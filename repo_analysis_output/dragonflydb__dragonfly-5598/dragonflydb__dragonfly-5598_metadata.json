{
    "primary_language": "C++",
    "language_guidelines": "Programming Language guidelines for C++:\n# General Guidelines : \n**General Guidelines for C/C++ Projects**\n\n1. **Read the README**  \n   Start by reading the project's README file on GitHub. It contains important instructions for installation, usage, and project-specific details.  \n\n2. **Check Dependencies**  \n   Look for dependencies listed in the README or package management files like `CMakeLists.txt` (for CMake), `Makefile` (for Make), or `vcpkg.json` (for vcpkg). Ensure the required compiler and libraries are installed.  \n\n3. **Build Tool**  \n   Identify the build tool the project is using: Make, CMake, or another. This information should be available in the README or through project configuration files (e.g., `Makefile` for Make, `CMakeLists.txt` for CMake).  \n\n4. **Build the Project**  \n   Depending on the build tool, use the appropriate commands to build the project:  \n\n   - For Make:  \n     ```  \n     make  \n     ```  \n   - For CMake:  \n     ```  \n     mkdir build  \n     cd build  \n     cmake ..  \n     make  \n     ```  \n\n5. **Configuration**  \n   Check if the project requires configuration files (e.g., `.conf` files, `config.h` headers) to be set up. This may involve providing paths to dependencies or setting compilation flags.  \n\n6. **Run Tests (if available)**  \n   If the project provides tests, it's a good idea to run them to ensure everything is working correctly. Common testing frameworks for C/C++ include Google Test (gtest), Catch2, and Boost.Test.  \n   - For Google Test:  \n     ```  \n     ./path/to/test_executable  \n     ```  \n   - For Catch2:  \n     ```  \n     ./path/to/test_executable  \n     ```  \n\n7. **Run the Project**  \n   Follow the instructions in the README to run the project. This could involve running a specific executable, starting a server, or executing a specific command.  \n\n8. **Troubleshooting**  \n   If you encounter issues during installation or while running the project, refer to the project's issue tracker on GitHub or search for similar issues.  \n\n9. **Documentation**  \n   Review additional documentation such as Doxygen files, API documentation, or inline comments in the code. Understanding the documentation provides better insights into the project\u2019s structure and usage.  ",
    "workflow_guidelines": "\nThe following workflow files might contain information on how to set up the project and run test cases.  This might be useful later on when building/installing and testing the project:\n\nfile: bullmq-tests.yml\nContent:\n# These tests are disabled until Dragonfly works well with BullMQ.\nname: bullmq-tests\non:\n  workflow_dispatch:\n\npermissions:\n  contents: read\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    name: Build\n\n    timeout-minutes: 60\n\n    # TODO: Build Dragonfly instead of using a container, see below\n    services:\n      dragonflydb:\n        image: ghcr.io/dragonflydb/dragonfly-weekly:latest\n        env:\n          DFLY_cluster_mode: emulated\n          DFLY_lock_on_hashtags: true\n          HEALTHCHECK_PORT: 6379\n        ports:\n          - 6379:6379\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          submodules: true\n\n      - name: Install NodeJs\n        run: |\n          wget -q https://unofficial-builds.nodejs.org/download/release/v22.12.0/node-v22.12.0-linux-x64-glibc-217.tar.xz\n          tar -xf node-v22.12.0-linux-x64-glibc-217.tar.xz\n          sudo cp -r node-v22.12.0-linux-x64-glibc-217/* /usr/local/\n          sudo apt install yarn jq\n          node --version\n          npm --version\n          yarn --version\n          mkdir -p $GITHUB_WORKSPACE/build\n\n#      - name: Build Dragonfly\n#        run: |\n#          mkdir $GITHUB_WORKSPACE/build\n#          cd $GITHUB_WORKSPACE/build\n#          cmake .. -DCMAKE_BUILD_TYPE=Release -GNinja\n#          ninja dragonfly\n#          ./dragonfly --alsologtostderr --cluster_mode=emulated --lock_on_hashtags --dbfilename= &\n\n      - name: Build BullMQ\n        run: |\n          mkdir -p $GITHUB_WORKSPACE/../bullmq\n          cd $GITHUB_WORKSPACE/../bullmq\n          # TODO: Use BullMQ latest release instead of our fork:\n          # DOWNLOAD_URL=$(curl -s https://api.github.com/repos/taskforcesh/bullmq/releases/latest | jq -r '.tarball_url')\n          # echo \"Downloading latest BullMQ release from ${DOWNLOAD_URL}\"\n          # wget -O bullmq.tar.gz ${DOWNLOAD_URL}\n          # tar -zxvf bullmq.tar.gz\n          # mv taskforcesh-bullmq-* bullmq\n          git clone https://github.com/dragonflydb/bullmq\n          cd bullmq\n          pwd\n          yarn install\n          yarn build\n\n      - name: Run BullMQ tests\n        run: |\n          cd $GITHUB_WORKSPACE/../bullmq/bullmq\n          BULLMQ_TEST_PREFIX={b} yarn test\n\n      - name: Upload logs on failure\n        if: failure()\n        uses: actions/upload-artifact@v4\n        with:\n          name: unit_logs\n          path: /tmp/dragonfly.*\n\nfile: ci.yml\nContent:\nname: ci-tests\n\non:\n  # push:\n  # branches: [ main ]\n  pull_request:\n    branches: [main]\n  workflow_dispatch:\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}\n  cancel-in-progress: true\n\njobs:\n  pre-commit:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          fetch-depth: 2\n      - uses: actions/setup-python@v5\n        with:\n            python-version: '3.x'\n            check-latest: true\n      - name: Install dependencies\n        run: |\n          python -m venv venv\n          source venv/bin/activate\n          python -m pip install pre-commit\n          lsblk -l\n          echo \"sda rotational = $(cat /sys/block/sda/queue/rotational)\"\n          echo \"sdb rotational = $(cat /sys/block/sdb/queue/rotational)\"\n      - name: Run pre-commit checks\n        run: |\n          source venv/bin/activate\n          pre-commit run --show-diff-on-failure --color=always --from-ref HEAD^ --to-ref HEAD\n        shell: bash\n  build:\n    strategy:\n      matrix:\n        # Test of these containers\n        container: [\"ubuntu-dev:20\", \"alpine-dev:latest\"]\n        build-type: [Debug, Release]\n        compiler: [{ cxx: g++, c: gcc }]\n        # -no-pie to disable address randomization so we could symbolize stacktraces\n        cxx_flags: [\"-Werror -no-pie\"]\n        sanitizers: [\"NoSanitizers\"]\n        include:\n          - container: \"alpine-dev:latest\"\n            build-type: Debug\n            compiler: { cxx: clang++, c: clang }\n            cxx_flags: \"\"\n            sanitizers: \"NoSanitizers\"\n          - container: \"ubuntu-dev:24\"\n            build-type: Debug\n            compiler: { cxx: clang++, c: clang }\n            # https://maskray.me/blog/2023-08-25-clang-wunused-command-line-argument (search for compiler-rt)\n            cxx_flags: \"-Wno-error=unused-command-line-argument\"\n            sanitizers: \"Sanitizers\"\n\n    runs-on: ubuntu-latest\n    container:\n      image: ghcr.io/romange/${{ matrix.container }}\n      # Seems that docker by default prohibits running iouring syscalls\n      options: --security-opt seccomp=unconfined --sysctl \"net.ipv6.conf.all.disable_ipv6=0\"\n      volumes:\n        - /:/hostroot\n        - /mnt:/mnt\n      credentials:\n        username: ${{ github.repository_owner }}\n        password: ${{ secrets.GITHUB_TOKEN }}\n\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          submodules: true\n\n      - name: Prepare Environment\n        run: |\n          uname -a\n          cmake --version\n          mkdir -p ${GITHUB_WORKSPACE}/build\n          mount\n\n          echo \"===================Before freeing up space ============================================\"\n          df -h\n          rm -rf /hostroot/usr/share/dotnet\n          rm -rf /hostroot/usr/local/share/boost\n          rm -rf /hostroot/usr/local/lib/android\n          rm -rf /hostroot/opt/ghc\n          echo \"===================After freeing up space ============================================\"\n          df -h\n          touch /mnt/foo\n          ls -la /mnt/foo\n\n      - name: Configure CMake\n        # Configure CMake in a 'build' subdirectory. `CMAKE_BUILD_TYPE` is only required if you are using a single-configuration generator such as make.\n        # See https://cmake.org/cmake/help/latest/variable/CMAKE_BUILD_TYPE.html?highlight=cmake_build_type\n        run: |\n          echo \"ulimit is\"\n          ulimit -s\n          echo \"-----------------------------\"\n          echo \"disk space is:\"\n          df -h\n          echo \"-----------------------------\"\n\n          export ASAN=\"OFF\"\n          export USAN=\"OFF\"\n\n          if [ '${{matrix.sanitizers}}' = 'Sanitizers' ]; then\n            echo \"ASAN/USAN\"\n            export ASAN=\"ON\"\n            export USAN=\"ON\"\n          fi\n\n          cmake -B ${GITHUB_WORKSPACE}/build \\\n            -DCMAKE_BUILD_TYPE=${{matrix.build-type}} \\\n            -DWITH_AWS=OFF \\\n            -DWITH_GCP=OFF \\\n            -DWITH_UNWIND=OFF \\\n            -DWITH_GPERF=OFF \\\n            -GNinja \\\n            -DCMAKE_C_COMPILER=\"${{matrix.compiler.c}}\" \\\n            -DCMAKE_CXX_COMPILER=\"${{matrix.compiler.cxx}}\" \\\n            -DCMAKE_CXX_FLAGS=\"${{matrix.cxx_flags}}\" -DWITH_AWS:BOOL=OFF \\\n            -DWITH_ASAN=\"${ASAN}\" \\\n            -DWITH_USAN=\"${USAN}\" \\\n            -L\n\n          cd ${GITHUB_WORKSPACE}/build && pwd\n          du -hcs _deps/\n\n      - name: Build\n        run: |\n          cd ${GITHUB_WORKSPACE}/build\n          ninja search_family_test\n          df -h\n          echo \"-----------------------------\"\n          ninja src/all\n\n      - name: PostFail\n        if: failure()\n        run: |\n          echo \"disk space is:\"\n          df -h\n\n      - name: C++ Unit Tests - IoUring\n        run: |\n          cd ${GITHUB_WORKSPACE}/build\n          echo Run ctest -V -L DFLY\n\n          GLOG_alsologtostderr=1 GLOG_vmodule=rdb_load=1,rdb_save=1,snapshot=1,op_manager=1,op_manager_test=1 \\\n          FLAGS_fiber_safety_margin=4096 timeout 20m ctest -V -L DFLY -E allocation_tracker_test\n\n          # Run allocation tracker test separately without alsologtostderr because it generates a TON of logs.\n          FLAGS_fiber_safety_margin=4096 timeout 5m ./allocation_tracker_test\n\n          timeout 5m ./dragonfly_test\n          timeout 5m ./json_family_test --jsonpathv2=false\n          timeout 5m ./tiered_storage_test --vmodule=db_slice=2 --logtostderr\n          timeout 5m ./search_test --use_numeric_range_tree=false\n          timeout 5m ./search_family_test --use_numeric_range_tree=false\n\n\n      - name: C++ Unit Tests - Epoll\n        run: |\n          cd ${GITHUB_WORKSPACE}/build\n\n          # Create a rule that automatically prints stacktrace upon segfault\n          cat > ./init.gdb <<EOF\n          catch signal SIGSEGV\n          command\n          bt\n          end\n          EOF\n\n          gdb -ix ./init.gdb --batch -ex r --args ./dragonfly_test --force_epoll\n          GLOG_alsologtostderr=1 FLAGS_fiber_safety_margin=4096 FLAGS_force_epoll=true GLOG_vmodule=rdb_load=1,rdb_save=1,snapshot=1 \\\n          timeout 20m ctest -V -L DFLY -E allocation_tracker_test\n\n          FLAGS_fiber_safety_margin=4096 FLAGS_force_epoll=true timeout 5m ./allocation_tracker_test\n\n      - name: C++ Unit Tests - IoUring with cluster mode\n        run: |\n          FLAGS_fiber_safety_margin=4096 FLAGS_cluster_mode=emulated timeout 20m ctest -V -L DFLY\n\n      - name: C++ Unit Tests - IoUring with cluster mode and FLAGS_lock_on_hashtags\n        run: |\n          FLAGS_fiber_safety_margin=4096 FLAGS_cluster_mode=emulated FLAGS_lock_on_hashtags=true timeout 20m ctest -V -L DFLY\n\n      - name: Upload unit logs on failure\n        if: failure()\n        uses: actions/upload-artifact@v4\n        with:\n          name: unit_logs\n          path: /tmp/*INFO*\n\n      - name: Run regression tests\n        if: matrix.container == 'ubuntu-dev:20'\n        uses: ./.github/actions/regression-tests\n        with:\n          dfly-executable: dragonfly\n          run-only-on-ubuntu-latest: true\n          build-folder-name: build\n          # Non-release build will not run tests marked as slow or opt_only\n          # \"not empty\" string is needed for release build because pytest command can not get empty string for filter\n          filter: ${{ matrix.build-type == 'Release' && 'not empty' || '(not slow) and (not opt_only)' }}\n\n      - name: Upload regression logs on failure\n        if: failure()\n        uses: actions/upload-artifact@v4\n        with:\n          name: regression_logs\n          path: /tmp/failed/*\n\n  lint-test-chart:\n    runs-on: ubuntu-latest\n    needs: [build]\n    steps:\n      - uses: actions/checkout@v4\n      - uses: ./.github/actions/lint-test-chart\n\nfile: daily-builds.yml\nContent:\nname: daily-builds\n\non:\n  schedule:\n    - cron: '0 6 * * *' # run at 6 AM UTC\n  workflow_dispatch:\n\njobs:\n  build:\n    # The CMake configure and build commands are platform agnostic and should work equally\n    # well on Windows or Mac.  You can convert this to a matrix build if you need\n    # cross-platform coverage.\n    # See: https://docs.github.com/en/free-pro-team@latest/actions/learn-github-actions/managing-complex-workflows#using-a-build-matrix\n    runs-on: ubuntu-latest\n    name: Build ${{ matrix.name }}\n    strategy:\n      matrix:\n        include:\n          # Build with these flags\n          - name: generic\n            container: alpine-dev\n            flags: \"-DMARCH_OPT=-march=x86-64\"\n          - name: fedora\n            container: fedora:30\n\n    timeout-minutes: 45\n\n    container:\n      image: ghcr.io/romange/${{ matrix.container }}\n      options: --security-opt seccomp=unconfined\n      credentials:\n        username: ${{ github.repository_owner }}\n        password: ${{ secrets.GITHUB_TOKEN }}\n\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          submodules: true\n\n      - name: Install dependencies\n        run: |\n          cmake --version\n          mkdir -p $GITHUB_WORKSPACE/build\n      - name: Install packages\n        if: matrix.container == 'fedora:30'\n        run: |\n          echo Passed\n\n      - name: Configure & Build\n        run: |\n          cd $GITHUB_WORKSPACE/build\n          cmake .. -DCMAKE_BUILD_TYPE=Debug -GNinja ${{ matrix.flags }}\n          ninja src/all\n      - name: Test\n        run: |\n            cd $GITHUB_WORKSPACE/build\n            ctest -V -L DFLY\n\n      - name: Send notification on failure\n        if: failure() && github.ref == 'refs/heads/main'\n        run: |\n          job_link=\"${GITHUB_SERVER_URL}/${GITHUB_REPOSITORY}/actions/runs/${GITHUB_RUN_ID}\"\n          message=\"Daily build (${{ matrix.name }}) failed.\\\\n Commit: ${{github.sha}}\\\\n Job Link: ${job_link}\\\\n\"\n\n          curl -s \\\n            -X POST \\\n            -H 'Content-Type: application/json' \\\n            '${{ secrets.GSPACES_BOT_DF_BUILD }}' \\\n            -d '{\"text\": \"'\"${message}\"'\"}'\n\n  build-macos:\n    runs-on: macos-14\n    timeout-minutes: 45\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          submodules: true\n\n      - name: Install dependencies\n        run: |\n\n          # Remove Python3 symlinks in /usr/local/bin as workaround to brew update issues\n          # https://github.com/actions/setup-python/issues/577\n          rm /usr/local/bin/2to3* || :\n          rm /usr/local/bin/idle3* || :\n          rm /usr/local/bin/pydoc* || :\n          rm /usr/local/bin/python3* || :\n          brew update && brew install ninja boost automake zstd bison autoconf libtool\n\n          mkdir -p $GITHUB_WORKSPACE/build\n\n      - name: Configure & Build\n        run: |\n          cd $GITHUB_WORKSPACE/build\n\n          export PATH=/opt/homebrew/bin:$PATH\n          export PATH=/opt/homebrew/opt/bison/bin/:$PATH\n\n          which bison\n          bison --version\n\n          gcc-12 --version\n\n          autoconf --help\n          autoreconf --help\n\n          echo \"*************************** START BUILDING **************************************\"\n          CC=gcc-12 CXX=g++-12 cmake .. -DCMAKE_BUILD_TYPE=Debug -GNinja -DWITH_UNWIND=OFF \\\n            -DCMAKE_CXX_FLAGS=\"-Wl,-ld_classic\" \\\n            -DCMAKE_C_COMPILER=\"gcc-12\" -DCMAKE_CXX_COMPILER=\"g++-12\"\n\n          ninja src/all\n\n      - name: Test\n        run: |\n            cd $GITHUB_WORKSPACE/build\n            ctest -V -L DFLY\n\n      - name: Send notification on failure\n        if: failure() && github.ref == 'refs/heads/main'\n        run: |\n          job_link=\"${GITHUB_SERVER_URL}/${GITHUB_REPOSITORY}/actions/runs/${GITHUB_RUN_ID}\"\n          message=\"Daily build (macOS) failed.\\\\n Commit: ${{github.sha}}\\\\n Job Link: ${job_link}\\\\n\"\n\n          curl -s \\\n            -X POST \\\n            -H 'Content-Type: application/json' \\\n            '${{ secrets.GSPACES_BOT_DF_BUILD }}' \\\n            -d '{\"text\": \"'\"${message}\"'\"}'\n\nfile: epoll-regression-tests.yml\nContent:\nname: Epoll Regression Tests\n\non:\n  schedule:\n    - cron: \"0 0/3 * * *\"\n  workflow_dispatch:\n\njobs:\n  build:\n    strategy:\n      matrix:\n        # Test of these containers\n        container: [\"ubuntu-dev:20\"]\n        proactor: [Epoll]\n        build-type: [Debug]\n        runner: [ubuntu-latest, [self-hosted, linux, ARM64]]\n\n    runs-on: ${{ matrix.runner }}\n\n    container:\n      image: ghcr.io/romange/${{ matrix.container }}\n      options: --security-opt seccomp=unconfined --sysctl \"net.ipv6.conf.all.disable_ipv6=0\"\n      volumes:\n        - /var/crash:/var/crash\n\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          submodules: true\n\n      - name: Print environment info\n        run: |\n          cat /proc/cpuinfo\n          ulimit -a\n          env\n\n      - name: Configure & Build\n        run: |\n          # -no-pie to disable address randomization so we could symbolize stacktraces\n          cmake -B ${GITHUB_WORKSPACE}/build -DCMAKE_BUILD_TYPE=${{matrix.build-type}} -GNinja \\\n                -DCMAKE_CXX_COMPILER_LAUNCHER=ccache -DPRINT_STACKTRACES_ON_SIGNAL=ON \\\n                -DCMAKE_CXX_FLAGS=-no-pie -DHELIO_STACK_CHECK:STRING=4096\n\n          cd ${GITHUB_WORKSPACE}/build  && ninja dragonfly\n          pwd\n          ls -l ..\n\n      - name: Run regression tests action\n        uses: ./.github/actions/regression-tests\n        with:\n          dfly-executable: dragonfly\n          gspace-secret: ${{ secrets.GSPACES_BOT_DF_BUILD }}\n          build-folder-name: build\n          filter: ${{ matrix.build-type == 'Release' && 'not empty' || 'not opt_only' }}\n          aws-access-key-id: ${{ secrets.AWS_S3_ACCESS_KEY }}\n          aws-secret-access-key: ${{ secrets.AWS_S3_ACCESS_SECRET }}\n          s3-bucket: ${{ secrets.S3_REGTEST_BUCKET }}\n          # Chain ternary oprator of the form (which can be nested)\n          # (expression == condition && <true expression> || <false expression>)\n          epoll: ${{ matrix.proactor == 'Epoll' && 'epoll' || 'iouring' }}\n\n      - name: Upload logs on failure\n        if: failure()\n        uses: actions/upload-artifact@v4\n        with:\n          name: logs\n          path: /tmp/failed/*\n\n      - name: Copy binary on a self hosted runner\n        if: failure()\n        run: |\n          # We must use sh syntax.\n          if [ \"$RUNNER_ENVIRONMENT\" = \"self-hosted\" ]; then\n            cd ${GITHUB_WORKSPACE}/build\n            timestamp=$(date +%Y-%m-%d_%H:%M:%S)\n            mv ./dragonfly /var/crash/dragonfy_${timestamp}\n          fi\n\n  lint-test-chart:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: ./.github/actions/lint-test-chart\n\nfile: mastodon-ruby-tests.yml\nContent:\nname: Mastodon ruby tests\non:\n  schedule:\n    - cron: '0 6 * * *' # run at 6 AM UTC\n  workflow_dispatch:\n\n\njobs:\n  build-and-test:\n    runs-on: ubuntu-latest\n    name: Build and run tests\n\n    services:\n      postgres:\n        image: postgres:14-alpine\n        env:\n          POSTGRES_PASSWORD: postgres\n          POSTGRES_USER: postgres\n        options: >-\n          --health-cmd pg_isready\n          --health-interval 10ms\n          --health-timeout 3s\n          --health-retries 50\n        ports:\n          - 5432:5432\n\n      redis:\n        image: docker.dragonflydb.io/dragonflydb/dragonfly:latest\n        options: >-\n          --health-cmd \"redis-cli ping\"\n          --health-interval 10ms\n          --health-timeout 3s\n          --health-retries 50\n        ports:\n          - 6379:6379\n\n    env:\n      DB_HOST: localhost\n      DB_USER: postgres\n      DB_PASS: postgres\n      RAILS_ENV: test\n      ALLOW_NOPAM: true\n      PAM_ENABLED: true\n      PAM_DEFAULT_SERVICE: pam_test\n      PAM_CONTROLLED_SERVICE: pam_test_controlled\n      OIDC_ENABLED: true\n      OIDC_SCOPE: read\n      SAML_ENABLED: true\n      CAS_ENABLED: true\n      BUNDLE_WITH: 'pam_authentication test'\n      GITHUB_RSPEC: false\n\n    steps:\n      - name: Checkout mastodon\n        uses: actions/checkout@v4\n        with:\n          repository: mastodon/mastodon\n      - name: Install pre-requisites\n        run: |\n          sudo apt update\n          sudo apt install -y libicu-dev libidn11-dev libvips42 ffmpeg imagemagick libpam-dev\n      - name: Set up Ruby\n        uses: ruby/setup-ruby@v1\n        with:\n          ruby-version: 3.4\n          bundler-cache: true\n      - name: Set up Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version-file: '.nvmrc'\n      - name: Enable corepack\n        shell: bash\n        run: corepack enable\n      - name: Install all production yarn packages\n        shell: bash\n        run: yarn workspaces focus --production\n      - name: Precompile assets\n        run: |-\n          bin/rails assets:precompile\n      - name: Load database schema\n        run: |\n          bin/rails db:setup\n          bin/flatware fan bin/rails db:test:prepare\n      - name: Run tests\n        env:\n          SPEC_OPTS: '--exclude-pattern \"**/self_destruct_scheduler_spec.rb\"'\n        run: |\n          unset COVERAGE\n          bin/flatware rspec -r ./spec/flatware_helper.rb\n      - name: Notify on failures\n        if: failure()\n        shell: bash\n        run: |\n          job_link=\"${GITHUB_SERVER_URL}/${GITHUB_REPOSITORY}/actions/runs/${GITHUB_RUN_ID}\"\n          message=\"Mastodon ruby tests failed.\\\\n The commit is: ${{github.sha}}.\\\\n Job Link: ${job_link}\\\\n\"\n          curl -s \\\n            -X POST \\\n            -H 'Content-Type: application/json' \\\n            '${{ secrets.GSPACES_BOT_DF_BUILD }}' \\\n            -d '{\"text\": \"'\"${message}\"'\"}'\n\nfile: regression-tests.yml\nContent:\nname: Regression Tests\n\non:\n  schedule:\n    - cron: \"0 0/3 * * *\"\n  workflow_dispatch:\n\njobs:\n  build:\n    strategy:\n      matrix:\n        # Test of these containers\n        container: [\"ubuntu-dev:20\"]\n        proactor: [Uring]\n        build-type: [Debug, Release]\n        runner: [ubuntu-latest, [self-hosted, linux, ARM64]]\n\n    runs-on: ${{ matrix.runner }}\n\n    container:\n      image: ghcr.io/romange/${{ matrix.container }}\n      options: --security-opt seccomp=unconfined --sysctl \"net.ipv6.conf.all.disable_ipv6=0\"\n      volumes:\n        - /var/crash:/var/crash\n\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          submodules: true\n\n      - name: Print environment info\n        run: |\n          cat /proc/cpuinfo\n          ulimit -a\n          env\n\n      - name: Configure & Build\n        run: |\n          # -no-pie to disable address randomization so we could symbolize stacktraces\n          cmake -B ${GITHUB_WORKSPACE}/build -DCMAKE_BUILD_TYPE=${{matrix.build-type}} -GNinja \\\n                -DCMAKE_CXX_COMPILER_LAUNCHER=ccache -DPRINT_STACKTRACES_ON_SIGNAL=ON \\\n                -DCMAKE_CXX_FLAGS=-no-pie -DHELIO_STACK_CHECK:STRING=4096\n\n          cd ${GITHUB_WORKSPACE}/build  && ninja dragonfly\n          pwd\n          ls -l ..\n\n      - name: Run regression tests action\n        uses: ./.github/actions/regression-tests\n        with:\n          dfly-executable: dragonfly\n          gspace-secret: ${{ secrets.GSPACES_BOT_DF_BUILD }}\n          build-folder-name: build\n          filter: ${{ matrix.build-type == 'Release' && 'not empty' || 'not opt_only' }}\n          aws-access-key-id: ${{ secrets.AWS_S3_ACCESS_KEY }}\n          aws-secret-access-key: ${{ secrets.AWS_S3_ACCESS_SECRET }}\n          s3-bucket: ${{ secrets.S3_REGTEST_BUCKET }}\n\n      - name: Upload logs on failure\n        if: failure()\n        uses: actions/upload-artifact@v4\n        with:\n          name: logs\n          path: /tmp/failed/*\n\n      - name: Copy binary on a self hosted runner\n        if: failure()\n        run: |\n          # We must use sh syntax.\n          if [ \"$RUNNER_ENVIRONMENT\" = \"self-hosted\" ]; then\n            cd ${GITHUB_WORKSPACE}/build\n            timestamp=$(date +%Y-%m-%d_%H:%M:%S)\n            mv ./dragonfly /var/crash/dragonfy_${timestamp}\n          fi\n\n  lint-test-chart:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: ./.github/actions/lint-test-chart\n\nfile: test-fakeredis.yml\nContent:\n---\nname: Test Dragonfly/Fakeredis\n\non:\n  workflow_dispatch:\n  pull_request:\n\npermissions:\n  contents: read\n  checks: write\n\nconcurrency:\n  group: dragonfly-${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    container:\n      image: ghcr.io/romange/ubuntu-dev:22\n      options: --security-opt seccomp=unconfined --sysctl \"net.ipv6.conf.all.disable_ipv6=0\"\n    strategy:\n      fail-fast: false\n    name: \"Run tests: \"\n    permissions:\n      pull-requests: write\n      checks: read\n\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          submodules: true\n\n      - name: Install dependencies\n        env:\n          PYTHON_KEYRING_BACKEND: keyring.backends.null.Keyring\n        shell: bash\n        working-directory: tests/fakeredis\n        run: |\n          pip install poetry\n          echo \"$HOME/.poetry/bin\" >> $GITHUB_PATH\n          poetry install\n      - name: Configure CMake\n        run: |\n          cmake -B ${GITHUB_WORKSPACE}/build \\\n            -DCMAKE_BUILD_TYPE=Debug -DWITH_AWS:BOOL=OFF -DWITH_GCP:BOOL=OFF -DWITH_GPERF:BOOL=OFF \\\n            -GNinja -L\n          cd ${GITHUB_WORKSPACE}/build && pwd\n\n      - name: Build\n        run: |\n          cd ${GITHUB_WORKSPACE}/build\n          ninja dragonfly\n          echo \"-----------------------------\"\n\n          # The order of redirect is important\n          ./dragonfly --proactor_threads=4  --noversion_check --port=6380  \\\n           --lua_resp2_legacy_float 1> /tmp/dragonfly.log 2>&1 &\n\n      - name: Run tests\n        working-directory: tests/fakeredis\n        run: |\n          # Some tests are pending on #5383\n          poetry run pytest test/ \\\n          --ignore test/test_hypothesis/test_transaction.py \\\n          --ignore test/test_hypothesis/test_zset.py \\\n          --ignore test/test_hypotesis_joint/test_joint.py \\\n          --junit-xml=results-tests.xml  --html=report-tests.html -v\n        continue-on-error: false  # Fail the job if tests fail\n\n      - name: Show Dragonfly stats\n        if: always()\n        run: |\n          redis-cli -p 6380 INFO ALL\n      - name: Upload Tests Result xml\n        if: always()\n        uses: actions/upload-artifact@v4\n        with:\n          name: tests-result-logs\n          path: |\n            /tmp/dragonfly.*\n\n      - name: Upload Tests Result html\n        if: always()\n        uses: actions/upload-artifact@v4\n        with:\n          name: report-tests.html\n          path: tests/fakeredis/report-tests.html\n\n      - name: Publish Test Report\n        if: ${{ github.event_name == 'pull_request' }}\n        uses: mikepenz/action-junit-report@v5\n        with:\n          report_paths: tests/fakeredis/results-tests.xml\n          # Do not create a check run\n          # annotate_only: true\n\n  publish-html-results:\n    name: Publish HTML Test Results to GitHub Pages\n    needs: test\n    if: ${{ github.ref == 'refs/heads/main' }}\n    runs-on: ubuntu-latest\n    permissions:\n      pages: write      # to deploy to Pages\n      id-token: write   # to verify the deployment originates from an appropriate source\n    environment:\n      name: github-pages\n      url: ${{ steps.deployment.outputs.page_url }}\n    steps:\n      - name: Bundle Tests Result to one artifact\n        uses: actions/upload-artifact/merge@v4\n        with:\n          delete-merged: true\n          name: test-results-html\n          pattern: '*.html'\n\n      - name: Download html pages\n        uses: actions/download-artifact@v4\n        with:\n          name: test-results-html\n          path: results/\n\n      - uses: actions/setup-python@v5\n        with:\n          cache-dependency-path: tests/fakeredis/poetry.lock\n          python-version: \"3.10\"\n\n      - name: Merge html results\n        run: |\n          pip install pytest-html-merger && mkdir merged\n          pytest_html_merger -i results/ -o merged/index.html\n\n      - name: Publish to GitHub Pages\n        uses: actions/upload-pages-artifact@v3\n        with:\n          path: merged/\n      - name: Deploy to GitHub Pages\n        id: deployment\n        uses: actions/deploy-pages@v4\n        with:\n          token: '${{ secrets.GITHUB_TOKEN }}'\n",
    "readme": "<p align=\"center\">\n  <a href=\"https://dragonflydb.io\">\n    <img  src=\"/.github/images/logo-full.svg\"\n      width=\"284\" border=\"0\" alt=\"Dragonfly\">\n  </a>\n</p>\n\n[![ci-tests](https://github.com/dragonflydb/dragonfly/actions/workflows/ci.yml/badge.svg)](https://github.com/dragonflydb/dragonfly/actions/workflows/ci.yml) [![Twitter URL](https://img.shields.io/twitter/follow/dragonflydbio?style=social)](https://twitter.com/dragonflydbio)\n\n> Before moving on, please consider giving us a GitHub star \u2b50\ufe0f. Thank you!\n\nOther languages:  [\u7b80\u4f53\u4e2d\u6587](README.zh-CN.md) [\u65e5\u672c\u8a9e](README.ja-JP.md) [\ud55c\uad6d\uc5b4](README.ko-KR.md) [Portugu\u00eas](README.pt-BR.md)\n\n[Website](https://www.dragonflydb.io/) \u2022 [Docs](https://dragonflydb.io/docs) \u2022 [Quick Start](https://www.dragonflydb.io/docs/getting-started) \u2022 [Community Discord](https://discord.gg/HsPjXGVH85) \u2022 [Dragonfly Forum](https://dragonfly.discourse.group/) \u2022 [Join the Dragonfly Community](https://www.dragonflydb.io/community)\n\n[GitHub Discussions](https://github.com/dragonflydb/dragonfly/discussions) \u2022 [GitHub Issues](https://github.com/dragonflydb/dragonfly/issues) \u2022 [Contributing](https://github.com/dragonflydb/dragonfly/blob/main/CONTRIBUTING.md) \u2022 [Dragonfly Cloud](https://www.dragonflydb.io/cloud)\n\n## The world's most efficient in-memory data store\n\nDragonfly is an in-memory data store built for modern application workloads.\n\nFully compatible with Redis and Memcached APIs, Dragonfly requires no code changes to adopt. Compared to legacy in-memory datastores, Dragonfly delivers 25X more throughput, higher cache hit rates with lower tail latency, and can run on up to 80% less resources for the same sized workload.\n\n## Contents\n\n- [Benchmarks](#benchmarks)\n- [Quick start](https://github.com/dragonflydb/dragonfly/tree/main/docs/quick-start)\n- [Configuration](#configuration)\n- [Roadmap and status](#roadmap-status)\n- [Design decisions](#design-decisions)\n- [Background](#background)\n- [Build from source](./docs/build-from-source.md)\n\n## <a name=\"benchmarks\"><a/>Benchmarks\n\nWe first compare Dragonfly with Redis on `m5.large` instance which is commonly used to run Redis\ndue to its single-threaded architecture. The benchmark program runs from another\nload-test instance (c5n) in the same AZ using `memtier_benchmark  -c 20 --test-time 100 -t 4 -d 256 --distinct-client-seed`\n\nDragonfly shows a comparable performance:\n\n1. SETs (`--ratio 1:0`):\n\n|  Redis                                   |      DF                                |\n| -----------------------------------------|----------------------------------------|\n| QPS: 159K, P99.9: 1.16ms, P99: 0.82ms    | QPS:173K, P99.9: 1.26ms, P99: 0.9ms    |\n|                                          |                                        |\n\n2. GETs (`--ratio 0:1`):\n\n|  Redis                                  |      DF                                |\n| ----------------------------------------|----------------------------------------|\n| QPS: 194K, P99.9: 0.8ms, P99: 0.65ms    | QPS: 191K, P99.9: 0.95ms, P99: 0.8ms   |\n\nThe benchmark above shows that the algorithmic layer inside DF that allows it to scale vertically\ndoes not take a large toll when running single-threaded.\n\nHowever, if we take a bit stronger instance (m5.xlarge), the gap between DF and Redis starts growing.\n(`memtier_benchmark  -c 20 --test-time 100 -t 6 -d 256 --distinct-client-seed`):\n1. SETs (`--ratio 1:0`):\n\n|  Redis                                  |      DF                                |\n| ----------------------------------------|----------------------------------------|\n| QPS: 190K, P99.9: 2.45ms, P99: 0.97ms   |  QPS: 279K , P99.9: 1.95ms, P99: 1.48ms|\n\n2. GETs (`--ratio 0:1`):\n\n|  Redis                                  |      DF                                |\n| ----------------------------------------|----------------------------------------|\n| QPS: 220K, P99.9: 0.98ms , P99: 0.8ms   |  QPS: 305K, P99.9: 1.03ms, P99: 0.87ms |\n\n\nDragonfly throughput capacity continues to grow with instance size,\nwhile single-threaded Redis is bottlenecked on CPU and reaches local maxima in terms of performance.\n\n<img src=\"http://static.dragonflydb.io/repo-assets/aws-throughput.svg\" width=\"80%\" border=\"0\"/>\n\nIf we compare Dragonfly and Redis on the most network-capable instance c6gn.16xlarge,\nDragonfly showed a 25X increase in throughput compared to Redis single process, crossing 3.8M QPS.\n\nDragonfly's 99th percentile latency metrics at its peak throughput:\n\n| op    | r6g   | c6gn  | c7g   |\n|-------|-------|-------|-------|\n| set   | 0.8ms | 1ms   | 1ms   |\n| get   | 0.9ms | 0.9ms | 0.8ms |\n| setex | 0.9ms | 1.1ms | 1.3ms |\n\n*All benchmarks were performed using `memtier_benchmark` (see below) with number of threads tuned per server and instance type. `memtier` was run on a separate c6gn.16xlarge machine. We set the expiry time to 500 for the SETEX benchmark to ensure it would survive the end of the test.*\n\n```bash\n  memtier_benchmark --ratio ... -t <threads> -c 30 -n 200000 --distinct-client-seed -d 256 \\\n     --expiry-range=...\n```\n\nIn pipeline mode `--pipeline=30`, Dragonfly reaches **10M QPS** for SET and **15M QPS** for GET operations.\n\n### Dragonfly vs. Memcached\n\nWe compared Dragonfly with Memcached on a c6gn.16xlarge instance on AWS.\n\nWith a comparable latency, Dragonfly throughput outperformed Memcached throughput in both write and read workloads. Dragonfly demonstrated better latency in write workloads due to contention on the [write path in Memcached](docs/memcached_benchmark.md).\n\n#### SET benchmark\n\n| Server    | QPS(thousands qps) | latency 99% | 99.9%   |\n|:---------:|:------------------:|:-----------:|:-------:|\n| Dragonfly |  \ud83d\udfe9 3844           |\ud83d\udfe9 0.9ms     | \ud83d\udfe9 2.4ms |\n| Memcached |   806              |   1.6ms     | 3.2ms    |\n\n#### GET benchmark\n\n| Server    | QPS(thousands qps) | latency 99% | 99.9%   |\n|-----------|:------------------:|:-----------:|:-------:|\n| Dragonfly | \ud83d\udfe9 3717            |   1ms       | 2.4ms   |\n| Memcached |   2100             |  \ud83d\udfe9 0.34ms  | \ud83d\udfe9 0.6ms |\n\n\nMemcached exhibited lower latency for the read benchmark, but also lower throughput.\n\n### Memory efficiency\n\nTo test memory efficiency, we filled Dragonfly and Redis with ~5GB of data using the `debug populate 5000000 key 1024` command, sent update traffic with `memtier`, and kicked off the snapshotting with the `bgsave` command.\n\nThis figure demonstrates how each server behaved in terms of memory efficiency.\n\n<img src=\"http://static.dragonflydb.io/repo-assets/bgsave-memusage.svg\" width=\"70%\" border=\"0\"/>\n\nDragonfly was 30% more memory efficient than Redis in the idle state and did not show any visible increase in memory use during the snapshot phase. At peak, Redis memory use increased to almost 3X that of Dragonfly.\n\nDragonfly finished the snapshot faster, within a few seconds.\n\nFor more info about memory efficiency in Dragonfly, see our [Dashtable doc](/docs/dashtable.md).\n\n\n\n## <a name=\"configuration\"><a/>Configuration\n\nDragonfly supports common Redis arguments where applicable. For example, you can run: `dragonfly --requirepass=foo --bind localhost`.\n\nDragonfly currently supports the following Redis-specific arguments:\n * `port`: Redis connection port (`default: 6379`).\n * `bind`: Use `localhost` to only allow localhost connections or a public IP address to allow connections **to that IP** address (i.e. from outside too). Use `0.0.0.0` to allow all IPv4.\n * `requirepass`: The password for AUTH authentication (`default: \"\"`).\n * `maxmemory`: Limit on maximum memory (in human-readable bytes) used by the database (`default: 0`). A `maxmemory` value of `0` means the program will automatically determine its maximum memory usage.\n * `dir`: Dragonfly Docker uses the `/data` folder for snapshotting by default, the CLI uses `\"\"`. You can use the `-v` Docker option to map it to your host folder.\n * `dbfilename`: The filename to save and load the database (`default: dump`).\n\nThere are also some Dragonfly-specific arguments:\n * `memcached_port`: The port to enable Memcached-compatible API on (`default: disabled`).\n * `keys_output_limit`: Maximum number of returned keys in `keys` command (`default: 8192`). Note that `keys` is a dangerous command. We truncate its result to avoid a blowup in memory use when fetching too many keys.\n * `dbnum`: Maximum number of supported databases for `select`.\n * `cache_mode`: See the [novel cache design](#novel-cache-design) section below.\n * `hz`: Key expiry evaluation frequency (`default: 100`). Lower frequency uses less CPU when idle at the expense of a slower eviction rate.\n * `snapshot_cron`: Cron schedule expression for automatic backup snapshots using standard cron syntax with the granularity of minutes (`default: \"\"`).\n   Here are some cron schedule expression examples below, and feel free to read more about this argument in our [documentation](https://www.dragonflydb.io/docs/managing-dragonfly/backups#the-snapshot_cron-flag).\n\n   | Cron Schedule Expression | Description                                |\n   |--------------------------|--------------------------------------------|\n   | `* * * * *`              | At every minute                            |\n   | `*/5 * * * *`            | At every 5th minute                        |\n   | `5 */2 * * *`            | At minute 5 past every 2nd hour            |\n   | `0 0 * * *`              | At 00:00 (midnight) every day              |\n   | `0 6 * * 1-5`            | At 06:00 (dawn) from Monday through Friday |\n\n * `primary_port_http_enabled`: Allows accessing HTTP console on main TCP port if `true` (`default: true`).\n * `admin_port`: To enable admin access to the console on the assigned port (`default: disabled`). Supports both HTTP and RESP protocols.\n * `admin_bind`: To bind the admin console TCP connection to a given address (`default: any`). Supports both HTTP and RESP protocols.\n * `admin_nopass`: To enable open admin access to console on the assigned port, without auth token needed (`default: false`). Supports both HTTP and RESP protocols.\n * `cluster_mode`: Cluster mode supported (`default: \"\"`). Currently supports only `emulated`.\n * `cluster_announce_ip`: The IP that cluster commands announce to the client.\n * `announce_port`: The port that cluster commands announce to the client, and to replication master.\n\n### Example start script with popular options:\n\n```bash\n./dragonfly-x86_64 --logtostderr --requirepass=youshallnotpass --cache_mode=true -dbnum 1 --bind localhost --port 6379 --maxmemory=12gb --keys_output_limit=12288 --dbfilename dump.rdb\n```\n\nArguments can be also provided via:\n * `--flagfile <filename>`: The file should list one flag per line, with equal signs instead of spaces for key-value flags. No quotes are needed for flag values.\n * Setting environment variables. Set `DFLY_x`, where `x` is the exact name of the flag, case sensitive.\n\nFor more options like logs management or TLS support, run `dragonfly --help`.\n\n## <a name=\"roadmap-status\"><a/>Roadmap and status\n\nDragonfly currently supports ~185 Redis commands and all Memcached commands besides `cas`. Almost on par with the Redis 5 API, Dragonfly's next milestone will be to stabilize basic functionality and implement the replication API. If there is a command you need that is not implemented yet, please open an issue.\n\nFor Dragonfly-native replication, we are designing a distributed log format that will support order-of-magnitude higher speeds.\n\nFollowing the replication feature, we will continue adding missing commands for Redis versions 3-6 APIs.\n\nPlease see our [Command Reference](https://dragonflydb.io/docs/category/command-reference) for the current commands supported by Dragonfly.\n\n## <a name=\"design-decisions\"><a/> Design decisions\n\n### Novel cache design\n\nDragonfly has a single, unified, adaptive caching algorithm that is simple and memory efficient.\n\nYou can enable caching mode by passing the `--cache_mode=true` flag. Once this mode is on, Dragonfly will evict items least likely to be stumbled upon in the future but only when it is near the `maxmemory` limit.\n\n### Expiration deadlines with relative accuracy\n\nExpiration ranges are limited to ~8 years.\n\nExpiration deadlines with millisecond precision (PEXPIRE, PSETEX, etc.) are rounded to the closest second **for deadlines greater than 2^28ms**, which has less than 0.001% error and should be acceptable for large ranges. If this is not suitable for your use case, get in touch or open an issue explaining your case.\n\nFor more detailed differences between Dragonfly expiration deadlines and Redis implementations, [see here](docs/differences.md).\n\n### Native HTTP console and Prometheus-compatible metrics\n\nBy default, Dragonfly allows HTTP access via its main TCP port (6379). That's right, you can connect to Dragonfly via Redis protocol and via HTTP protocol \u2014 the server recognizes the protocol automatically during the connection initiation. Go ahead and try it with your browser. HTTP access currently does not have much info but will include useful debugging and management info in the future.\n\nGo to the URL `:6379/metrics` to view Prometheus-compatible metrics.\n\nThe Prometheus exported metrics are compatible with the Grafana dashboard, [see here](tools/local/monitoring/grafana/provisioning/dashboards/dashboard.json).\n\n\nImportant! The HTTP console is meant to be accessed within a safe network. If you expose Dragonfly's TCP port externally, we advise you to disable the console with `--http_admin_console=false` or `--nohttp_admin_console`.\n\n\n## <a name=\"background\"><a/>Background\n\nDragonfly started as an experiment to see how an in-memory datastore could look if it was designed in 2022. Based on lessons learned from our experience as users of memory stores and engineers who worked for cloud companies, we knew that we need to preserve two key properties for Dragonfly: Atomicity guarantees for all operations and low, sub-millisecond latency over very high throughput.\n\nOur first challenge was how to fully utilize CPU, memory, and I/O resources using servers that are available today in public clouds. To solve this, we use [shared-nothing architecture](https://en.wikipedia.org/wiki/Shared-nothing_architecture), which allows us to partition the keyspace of the memory store between threads so that each thread can manage its own slice of dictionary data. We call these slices \"shards\". The library that powers thread and I/O management for shared-nothing architecture is open-sourced [here](https://github.com/romange/helio).\n\nTo provide atomicity guarantees for multi-key operations, we use the advancements from recent academic research. We chose the paper [\"VLL: a lock manager redesign for main memory database systems\u201d](https://www.cs.umd.edu/~abadi/papers/vldbj-vll.pdf) to develop the transactional framework for Dragonfly. The choice of shared-nothing architecture and VLL allowed us to compose atomic multi-key operations without using mutexes or spinlocks. This was a major milestone for our PoC and its performance stood out from other commercial and open-source solutions.\n\nOur second challenge was to engineer more efficient data structures for the new store. To achieve this goal, we based our core hashtable structure on the paper [\"Dash: Scalable Hashing on Persistent Memory\"](https://arxiv.org/pdf/2003.07302.pdf). The paper itself is centered around the persistent memory domain and is not directly related to main-memory stores, but it's still most applicable to our problem. The hashtable design suggested in the paper allowed us to maintain two special properties that are present in the Redis dictionary: The incremental hashing ability during datastore growth the ability to traverse the dictionary under changes using a stateless scan operation. In addition to these two properties, Dash is more efficient in CPU and memory use. By leveraging Dash's design, we were able to innovate further with the following features:\n * Efficient record expiry for TTL records.\n * A novel cache eviction algorithm that achieves higher hit rates than other caching strategies like LRU and LFU with **zero memory overhead**.\n * A novel **fork-less** snapshotting algorithm.\n\nOnce we had built the foundation for Dragonfly and [we were happy with its performance](#benchmarks), we went on to implement the Redis and Memcached functionality. We have to date implemented ~185 Redis commands (roughly equivalent to Redis 5.0 API) and 13 Memcached commands.\n\nAnd finally, <br>\n<em>Our mission is to build a well-designed, ultra-fast, cost-efficient in-memory datastore for cloud workloads that takes advantage of the latest hardware advancements. We intend to address the pain points of current solutions while preserving their product APIs and propositions.</em>\n",
    "org": "dragonflydb",
    "repo": "dragonfly",
    "number": 5598,
    "commit": "a895afc971597b9168f0e814d4ea4b7e54dbd8aa"
}
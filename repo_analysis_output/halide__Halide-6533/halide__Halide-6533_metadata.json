{
    "primary_language": "C++",
    "language_guidelines": "# General Guidelines : \n**General Guidelines for C/C++ Projects**\n\n1. **Read the README**  \n   Start by reading the project's README file on GitHub. It contains important instructions for installation, usage, and project-specific details.  \n\n2. **Check Dependencies**  \n   Look for dependencies listed in the README or package management files like `CMakeLists.txt` (for CMake), `Makefile` (for Make), or `vcpkg.json` (for vcpkg). Ensure the required compiler and libraries are installed.  \n\n3. **Build Tool**  \n   Identify the build tool the project is using: Make, CMake, or another. This information should be available in the README or through project configuration files (e.g., `Makefile` for Make, `CMakeLists.txt` for CMake).  \n\n4. **Build the Project**  \n   Depending on the build tool, use the appropriate commands to build the project:  \n\n   - For Make:  \n     ```  \n     make  \n     ```  \n   - For CMake:  \n     ```  \n     mkdir build  \n     cd build  \n     cmake ..  \n     make  \n     ```  \n\n5. **Configuration**  \n   Check if the project requires configuration files (e.g., `.conf` files, `config.h` headers) to be set up. This may involve providing paths to dependencies or setting compilation flags.  \n\n6. **Run Tests (if available)**  \n   If the project provides tests, it's a good idea to run them to ensure everything is working correctly. Common testing frameworks for C/C++ include Google Test (gtest), Catch2, and Boost.Test.  \n   - For Google Test:  \n     ```  \n     ./path/to/test_executable  \n     ```  \n   - For Catch2:  \n     ```  \n     ./path/to/test_executable  \n     ```  \n\n7. **Run the Project**  \n   Follow the instructions in the README to run the project. This could involve running a specific executable, starting a server, or executing a specific command.  \n\n8. **Troubleshooting**  \n   If you encounter issues during installation or while running the project, refer to the project's issue tracker on GitHub or search for similar issues.  \n\n9. **Documentation**  \n   Review additional documentation such as Doxygen files, API documentation, or inline comments in the code. Understanding the documentation provides better insights into the project\u2019s structure and usage.  ",
    "workflow_guidelines": "\nThe following workflow files might contain information on how to set up the project and run test cases.  This might be useful later on when building/installing and testing the project:\n\nfile: test.yml\nContent:\n# TODO (known issues)\n# - no GPU tests are attempted (probably not possible)\n# - cmake static builds aren't handled yet.\n# - arm32 and arm64 is build-only, no testing (qemu is too slow).\n#   Perhaps some limited testing instead of none?\n# - python is built+tested for x86-64 targets only (no arm or 32-bit)\n# - apps are skipped for x86-32, arm-32, arm-64\n#\n# TODO (stuff that could be usefully added, perhaps)\n# - build + test of WASM\n# - build + test of PyTorch\n#\n# TODO (GHA issues)\n# - GHA is occasionally flaky and some VMs just fail, but there isn't a way\n#   to restart just one of the jobs (it's currently all-or-none)\n\nname: Halide Presubmit Build + Test\non:\n  workflow_dispatch:\n    # inputs:\n    #   logLevel:\n    #     description: 'Log level'\n    #     required: true\n    #     default: 'warning'\n    #   tags:\n    #     description: 'Test scenario tags'\n\n  # pull_request:\n  #   # We don't want 'edited' (that's basically just the description, title, etc)\n  #   # We don't want 'review_requested' (that's redundant to the ones below for our purposes)\n  #   types: [opened, synchronize, reopened]\n  #   # TODO: do we want to limit this to certain filetypes?\n  #   # paths:\n  #   #   - '**.h'\n  #   #   - '**.c'\n  #   #   - '**.cpp'\n\njobs:\n  test_halide:\n    name: HL-${{matrix.llvm_version}}-${{matrix.target_arch}}-${{matrix.target_bits}}-${{matrix.target_os}}-${{matrix.build_tool}}\n    runs-on: ${{matrix.host_os}}\n    env:\n      CC: ${{matrix.cc}}\n      CXX: ${{matrix.cxx}}\n      LD: ${{matrix.ld}}\n\n    strategy:\n      fail-fast: false  # Keep running other jobs even if one fails\n      # free-tier projects (like Halide) get 20 concurrent tasks.\n      # The build matrix here has only 7 tasks -- should we limit it to fewer\n      # than that? Need to experiment.\n      # max-parallel: TBD   TODO\n      matrix:\n        # TODO: this matrix is probably overkill; we don't need to build every combination.\n        # (Some combinations are nonsensical and excluded via the 'exclude:' section below.)\n        target_arch: [x86, arm]\n        target_bits: [32, 64]\n        target_os: [linux, osx, windows]\n        llvm_version: [12]\n        build_tool: [cmake_shared]\n        # llvm_version: [10, 11, 12]  # TODO\n        # build_tool: [cmake_shared, make]  # TODO\n\n        # This section basically allows us to define additional values for\n        # each matrix entry, e.g. to map an llvm version number to the specific\n        # git branch that is needed.\n        include:\n          # - llvm_version: 10\n          #   llvm_branch: release/10.x\n          # - llvm_version: 11\n          #   llvm_branch: release/11.x\n          - llvm_version: 12\n            llvm_branch: master\n\n          # map things to the necessary host cross-compiler host\n          - target_os: osx\n            host_os: macos-10.15\n            cc: clang\n            cxx: clang++\n            ld: ld\n\n          - target_os: linux\n            host_os: ubuntu-18.04\n            # GHA has clang 6, 8, and 9 and GCC 7.4, 8.3, 9.2 preinstalled.\n            # We will explicitly choose gcc 7.x (even though the default is gcc 7.4)\n            # to ensure we match gcc versions with the arm crosscompiler.\n            cc: gcc-7\n            cxx: g++-7\n            ld: ld\n\n          - target_os: windows\n            host_os: windows-2019\n            cc: cl.exe\n            cxx: cl.exe\n            ld: ld.exe\n\n          - target_arch: x86\n            python_version: '3.7'\n            uses_python: true\n            run_tests: true\n\n          - target_bits: 32\n            # We don't build/test Python bindings on any 32-bit targets\n            uses_python: false\n\n          - target_arch: arm\n            # We don't build/test Python bindings on any ARM targets\n            uses_python: false\n            # Running our test suite (via e.g. QEMU) is too slow to be useful\n            # at present (> 5 hours on current GHA VMs). That said, we'll leave\n            # in the relevant code for now (disabled via this flag) in case\n            # it proves useful later.\n            run_tests: false\n\n        exclude:\n          - target_os: osx\n            target_arch: arm    # OSX is x86-only\n          - target_os: osx\n            target_bits: 32     # OSX is 64-bit only\n          - target_os: windows\n            target_arch: arm    # OSX is x86-only\n          - target_os: windows\n            build_tool: make    # Windows is CMake-only\n          - target_arch: arm\n            build_tool: make    # In this setup, all ARM builds require CMake\n\n    steps:\n    - uses: actions/checkout@v2\n      with:\n        path: 'halide'\n\n    - name: Configure Python\n      if: matrix.uses_python\n      uses: actions/setup-python@v1\n      with:\n        python-version: '${{matrix.python_version}}'\n        architecture: 'x64'\n\n    - name: Configure Ubuntu Host\n      if: startsWith(matrix.host_os, 'ubuntu')\n      shell: bash\n      run: |\n        sudo apt-get update\n\n        sudo apt-get install \\\n          doxygen \\\n          libjpeg-dev \\\n          libpng-dev \\\n          ninja-build\n\n    - name: Configure MacOS Host\n      if: startsWith(matrix.host_os, 'macos')\n      shell: bash\n      run: |\n        # coreutils is for gtimeout\n        brew install \\\n          coreutils \\\n          doxygen \\\n          jpeg \\\n          libpng \\\n          ninja\n\n    - name: Configure Windows Host\n      if: startsWith(matrix.host_os, 'windows')\n      shell: bash\n      run: |\n        if [[ ${{matrix.target_bits}} == 32 ]]; then\n          export VCPKG_DEFAULT_TRIPLET=x86-windows\n        else\n          export VCPKG_DEFAULT_TRIPLET=x64-windows\n        fi\n\n        vcpkg install \\\n          libjpeg-turbo \\\n          libpng \\\n          zlib\n\n    - name: Configure x86-32 Crosscompilation\n      if: matrix.target_os == 'linux' && matrix.target_arch == 'x86' && matrix.target_bits == 32\n      shell: bash\n      run: |\n        sudo dpkg --add-architecture i386\n        sudo apt-get update\n        sudo apt-get install \\\n          ${{matrix.cc}}-multilib \\\n          ${{matrix.cxx}}-multilib \\\n          libjpeg-dev:i386 \\\n          libpng-dev:i386 \\\n\n    - name: Configure Arm32 Crosscompilation\n      if: matrix.target_os == 'linux' && matrix.target_arch == 'arm' && matrix.target_bits == 32\n      shell: bash\n      run: |\n        # Note that we are configuring this for user-mode emulation:\n        # syscalls will be native, only user-mode code will be emulated.\n        # This is not 100% perfect (there are various corner cases that\n        # can bite us), but is *much* faster than full machine emulation.\n\n        sudo apt-get update\n        sudo apt-get install --install-suggests \\\n          ${{matrix.cc}}-arm-linux-gnueabihf \\\n          ${{matrix.cxx}}-arm-linux-gnueabihf\n\n        # TODO: figure out how to install libjpeg and libpng for armhf;\n        # the standard apt repository for GHA VMs barfs on these.\n        # sudo apt-get install \\\n        #   libjpeg-dev:armhf \\\n        #   libpng-dev:armhf\n\n        # Note that we need QEMU even if not running tests, as Generators\n        # will be built for arm by default, and we need to be able to run them.\n        sudo apt-get install --install-suggests \\\n          qemu-user \\\n          qemu-user-binfmt\n\n        qemu-arm --version\n        echo ::set-env name=QEMU_LD_PREFIX::\"/usr/arm-linux-gnueabihf\"\n\n    - name: Configure AArch64 Crosscompilation\n      if: matrix.target_os == 'linux' && matrix.target_arch == 'arm' && matrix.target_bits == 64\n      shell: bash\n      run: |\n        sudo apt-get update\n        sudo apt-get install --install-suggests \\\n          ${{matrix.cc}}-aarch64-linux-gnu \\\n          ${{matrix.cxx}}-aarch64-linux-gnu\n\n        # TODO: figure out how to install libjpeg and libpng for armhf;\n        # the standard apt repository for GHA VMs barfs on these.\n        # sudo apt-get install \\\n        #   libjpeg-dev:aarch64 \\\n        #   libpng-dev:aarch64\n\n        # Note that we need QEMU even if not running tests, as Generators\n        # will be built for arm by default, and we need to be able to run them.\n        sudo apt-get install --install-suggests \\\n          qemu-user \\\n          qemu-user-binfmt\n\n        qemu-arm --version\n        echo ::set-env name=QEMU_LD_PREFIX::\"/usr/aarch64-linux-gnu\"\n\n    - name: Configure Env Vars\n      shell: bash\n      run: |\n        echo \"github.event_name is ${{github.event_name}}\"  # should always be \"pull_request\"\n        echo \"github.event.action is ${{github.event.action}}\"\n\n        # Demangle Windows names, to simplify CMake stuff later\n        _ROOT=${GITHUB_WORKSPACE//\\\\//}\n        _TEMP_RAW=\"${{runner.temp}}\"\n        _TEMP=${_TEMP_RAW//\\\\//}\n\n        # This is the trick GitHub Actions uses to allow us to set env vars across all subsequent job steps\n        echo ::set-env name=BUILD_TYPE::\"Release\"\n        echo ::set-env name=LLVM_INSTALL_DIR::\"${_ROOT}/llvm\"\n        echo ::set-env name=LLVM_CONFIG::\"${_ROOT}/llvm/bin/llvm-config\"\n        echo ::set-env name=HALIDE_SOURCE_DIR::\"${_ROOT}/halide\"\n        echo ::set-env name=HALIDE_BUILD_DIR::\"${_ROOT}/halide_build\"\n        echo ::set-env name=HALIDE_TEMP_DIR::\"${_TEMP}\"\n        echo ::set-env name=PARALLEL_JOBS::\"4\"\n        if [[ ${{matrix.host_os}} == windows* ]]; then\n          # On Windows, it's just 'python', apparently\n          echo ::set-env name=PYTHON::\"python\"\n        else\n          echo ::set-env name=PYTHON::\"python${{matrix.python_version}}\"\n        fi\n\n    - name: Install Python Dependencies\n      if: matrix.uses_python\n      shell: bash\n      run: |\n        ${PYTHON} -m pip --version\n        ${PYTHON} -m pip install --upgrade pip\n        ${PYTHON} -m pip install -r ${HALIDE_SOURCE_DIR}/python_bindings/requirements.txt\n\n        echo ::set-env name=PYTHON::\"${PYTHON}\"\n\n    - name: Install LLVM\n      shell: bash\n      run: |\n        LLVM_ID=\"llvm-${{matrix.llvm_version}}-${{matrix.target_arch}}-${{matrix.target_bits}}-${{matrix.target_os}}\"\n\n        curl \\\n          --user llvm_user:${{secrets.LLVM_USER_PASSWORD}} \\\n          --output ${HALIDE_TEMP_DIR}/llvm-prebuilt.tgz \\\n          https://buildbot.halide-lang.org/llvm/${LLVM_ID}.tgz\n\n        TAR_CMD=\"tar\"\n        if [[ ${{matrix.host_os}} == windows* ]]; then\n          # Must use --force-local to avoid tar misinterpreting the : in\n          # a Windows pathname as a hostname.\n          TAR_CMD=\"tar --force-local\"\n        fi\n\n        mkdir ${LLVM_INSTALL_DIR}\n        ${TAR_CMD} -xf ${HALIDE_TEMP_DIR}/llvm-prebuilt.tgz -C ${LLVM_INSTALL_DIR}\n        rm -rf ${HALIDE_TEMP_DIR}/llvm-prebuilt.tgz\n\n        LLVM_COMMIT_HASH=`cat ${LLVM_INSTALL_DIR}/.halide_builder_llvm_commit`\n        echo \"Using LLVM v${{matrix.llvm_version}} commit=${LLVM_COMMIT_HASH}\"\n\n    - name: Configure Halide (Make)\n      if: startsWith(matrix.build_tool, 'make')\n      shell: bash\n      run: |\n        # Configure Make\n        mkdir ${HALIDE_BUILD_DIR}\n\n        if [[ ${{matrix.target_arch}} == x86 && \\\n              ${{matrix.target_os}} == linux && \\\n              ${{matrix.target_bits}} == 32 ]]; then\n          echo ::set-env name=CC::\"${CC} -m32\"\n          echo ::set-env name=CXX::\"${CXX} -m32\"\n          echo ::set-env name=LD::\"${LD} -melf_i386\"\n        fi\n\n    - name: Configure Halide (CMake)\n      if: startsWith(matrix.build_tool, 'cmake')\n      shell: bash\n      run: |\n        # Configure CMake\n        echo `cmake --version`\n\n        mkdir ${HALIDE_BUILD_DIR}\n\n        CMAKE_GEN=\"Ninja\"\n        EXTRA_CMAKE_FLAGS=\n\n        if [[ ${{matrix.host_os}} == windows* ]]; then\n          CMAKE_GEN=\"Visual Studio 16 2019\"\n\n          # CMAKE_TOOLCHAIN_FILE is necessary for CMake to find things installed by vcpkg\n          EXTRA_CMAKE_FLAGS=\"${EXTRA_CMAKE_FLAGS} \\\n            -D CMAKE_TOOLCHAIN_FILE=${VCPKG_INSTALLATION_ROOT//\\\\//}/scripts/buildsystems/vcpkg.cmake \\\n            -T host=x64\"\n          if [[ ${{matrix.target_bits}} == 32 ]]; then\n            EXTRA_CMAKE_FLAGS=\"${EXTRA_CMAKE_FLAGS} -A Win32\"\n          else\n            EXTRA_CMAKE_FLAGS=\"${EXTRA_CMAKE_FLAGS} -A x64\"\n          fi\n        fi\n\n        if [[ ${{matrix.target_arch}} == x86 && \\\n              ${{matrix.target_os}} == linux && \\\n              ${{matrix.target_bits}} == 32 ]]; then\n          # Assume host_os is ubuntu*\n          EXTRA_CMAKE_FLAGS=\"${EXTRA_CMAKE_FLAGS} \\\n            -D CMAKE_TOOLCHAIN_FILE=${HALIDE_SOURCE_DIR}/cmake/toolchain.linux-i386.cmake\"\n        fi\n\n        if [[ ${{matrix.target_os}} == osx ]]; then\n          # LLVM_ENABLE_SUPPORT_XCODE_SIGNPOSTS=OFF is needed for compatibility with older XCode versions\n          EXTRA_CMAKE_FLAGS=\"${EXTRA_CMAKE_FLAGS} \\\n                             -D LLVM_ENABLE_SUPPORT_XCODE_SIGNPOSTS=FORCE_OFF\"\n        fi\n\n        if [[ ${{matrix.target_arch}} == arm ]]; then\n          # The arm toolchain files default to \"gcc\"/\"g++\" with no version appended,\n          # but we installed specific versions, so be sure it can find those specific versions.\n          if [[ ${{matrix.target_bits}} == 32 ]]; then\n            export ARCH_FOR_TESTS=armv7-a\n            EXTRA_CMAKE_FLAGS=\"${EXTRA_CMAKE_FLAGS} \\\n              -D CMAKE_C_COMPILER=arm-linux-gnueabihf-${{matrix.cc}} \\\n              -D CMAKE_CXX_COMPILER=arm-linux-gnueabihf-${{matrix.cxx}} \\\n              -D CMAKE_TOOLCHAIN_FILE=${HALIDE_SOURCE_DIR}/cmake/toolchain.linux-arm32.cmake\"\n          else\n            export ARCH_FOR_TESTS=armv8-a\n            EXTRA_CMAKE_FLAGS=\"${EXTRA_CMAKE_FLAGS} \\\n              -D CMAKE_C_COMPILER=aarch64-linux-gnu-${{matrix.cc}} \\\n              -D CMAKE_CXX_COMPILER=aarch64-linux-gnu-${{matrix.cxx}} \\\n              -D CMAKE_TOOLCHAIN_FILE=${HALIDE_SOURCE_DIR}/cmake/toolchain.linux-aarch64.cmake\"\n          fi\n        fi\n\n        REQUIRE_LLVM_VERSION=\"${{matrix.llvm_version}}0\"\n        SHARED_LIBRARY=$([ ${{matrix.build_tool}} == \"cmake_shared\" ] && echo \"ON\" || echo \"OFF\")\n\n        if [[ \"${{matrix.uses_python}}\" == \"true\" ]]; then\n          EXTRA_CMAKE_FLAGS=\"${EXTRA_CMAKE_FLAGS} \\\n            -D WITH_PYTHON_BINDINGS=ON\"\n        else\n          EXTRA_CMAKE_FLAGS=\"${EXTRA_CMAKE_FLAGS} \\\n            -D WITH_PYTHON_BINDINGS=OFF\"\n        fi\n\n        cmake \\\n          -D CMAKE_BUILD_TYPE=${BUILD_TYPE} \\\n          -D LLVM_DIR=\"${LLVM_INSTALL_DIR}/lib/cmake/llvm\" \\\n          -D HALIDE_REQUIRE_LLVM_VERSION=\"${REQUIRE_LLVM_VERSION}\" \\\n          -D HALIDE_SHARED_LIBRARY=${SHARED_LIBRARY} \\\n          -G \"${CMAKE_GEN}\" \\\n          ${EXTRA_CMAKE_FLAGS} \\\n          -S \"${HALIDE_SOURCE_DIR}\" \\\n          -B \"${HALIDE_BUILD_DIR}\"\n\n    - name: Build Halide (Make)\n      if: startsWith(matrix.build_tool, 'make')\n      shell: bash\n      run: |\n        # Build Halide\n        cd ${HALIDE_BUILD_DIR}\n\n        BUILD_TARGETS=\"distrib build_tests\"\n        if [[ \"${{matrix.uses_python}}\" == \"true\" ]]; then\n          # build_apps requires the python bindings\n          BUILD_TARGETS=\"${BUILD_TARGETS} build_apps build_python_bindings\"\n        fi\n\n        make -f ${HALIDE_SOURCE_DIR}/Makefile -j ${PARALLEL_JOBS} ${BUILD_TARGETS}\n\n    - name: Build Halide (CMake)\n      if: startsWith(matrix.build_tool, 'cmake')\n      shell: bash\n      run: |\n        # Build Halide\n        cd ${HALIDE_BUILD_DIR}\n        cmake \\\n          --build ${HALIDE_BUILD_DIR} \\\n          --config ${BUILD_TYPE} \\\n          -j ${PARALLEL_JOBS}\n\n    - name: Run Tests (Make)\n      if: matrix.run_tests && startsWith(matrix.build_tool, 'make')\n      shell: bash\n      run: |\n        # Test Halide\n        export TEST_TMPDIR=\"${HALIDE_TEMP_DIR}\"\n        cd ${HALIDE_BUILD_DIR}\n\n        TEST_GROUPS_PARALLEL=\"internal correctness error warning generator\"\n        if [[ \"${{matrix.uses_python}}\" == \"true\" ]]; then\n          TEST_GROUPS_PARALLEL=\"${TEST_GROUPS_PARALLEL} python\"\n        fi\n\n        # tutorial has some performance measurements that can be flaky if we run them in parallel\n        TEST_GROUPS_SERIAL=\"tutorial\"\n\n        # performance is never going to be reliable on VMs.\n        # auto_schedule is just flaky.\n        TEST_GROUPS_BROKEN=\"performance auto_schedule\"\n\n        if [[ ${{matrix.target_bits}} == 32 ]]; then\n          # TODO: Skip testing apps on 32-bit systems for now;\n          # in particular, apps/autoscheduler can time out, and also has build\n          # issues on ubuntu-32 at the moment (__udivdi3).\n          TEST_GROUPS_BROKEN=\"${TEST_GROUPS_BROKEN} apps\"\n        else\n          TEST_GROUPS_PARALLEL=\"${TEST_GROUPS_PARALLEL} apps\"\n        fi\n\n        # Parallel\n        for t in ${TEST_GROUPS_PARALLEL}; do\n          make -f ${HALIDE_SOURCE_DIR}/Makefile -j ${PARALLEL_JOBS} test_${t}\n        done\n\n        # Serial\n        for t in ${TEST_GROUPS_SERIAL}; do\n          make -f ${HALIDE_SOURCE_DIR}/Makefile test_$t\n        done\n\n    - name: Run Tests (CMake)\n      if: matrix.run_tests && startsWith(matrix.build_tool, 'cmake')\n      shell: bash\n      run: |\n        # Test Halide\n        TEST_GROUPS_PARALLEL=\"internal|correctness|error|warning|generator\"\n\n        if [[ \"${{matrix.uses_python}}\" == \"true\" ]]; then\n          TEST_GROUPS_PARALLEL=\"${TEST_GROUPS_PARALLEL}|python\"\n        fi\n\n        # tutorial has some performance measurements that can be flaky if we run them in parallel\n        TEST_GROUPS_SERIAL=\"tutorial\"\n\n        # performance is never going to be reliable on VMs.\n        # auto_schedule is just flaky.\n        TEST_GROUPS_BROKEN=\"performance|auto_schedule\"\n\n        export TEST_TMPDIR=\"${HALIDE_TEMP_DIR}\"\n        cd ${HALIDE_BUILD_DIR}\n\n        # Parallel\n        ctest \\\n          -C ${BUILD_TYPE} \\\n          -j ${PARALLEL_JOBS} \\\n          -L \"${TEST_GROUPS_PARALLEL}\" \\\n          --output-on-failure\n\n        # Serial\n        ctest \\\n          -C ${BUILD_TYPE} \\\n          -L \"${TEST_GROUPS_SERIAL}\" \\\n          -E \"${TEST_GROUPS_BROKEN}\" \\\n          --output-on-failure\n",
    "readme": "# Halide\n\nHalide is a programming language designed to make it easier to write\nhigh-performance image and array processing code on modern machines. Halide\ncurrently targets:\n\n- CPU architectures: X86, ARM, MIPS, Hexagon, PowerPC, RISC-V\n- Operating systems: Linux, Windows, macOS, Android, iOS, Qualcomm QuRT\n- GPU Compute APIs: CUDA, OpenCL, OpenGL Compute Shaders, Apple Metal, Microsoft\n  Direct X 12\n\nRather than being a standalone programming language, Halide is embedded in C++.\nThis means you write C++ code that builds an in-memory representation of a\nHalide pipeline using Halide's C++ API. You can then compile this representation\nto an object file, or JIT-compile it and run it in the same process. Halide also\nprovides a Python binding that provides full support for writing Halide embedded\nin Python without C++.\n\nHalide requires C++17 (or later) to use.\n\nFor more detail about what Halide is, see http://halide-lang.org.\n\nFor API documentation see http://halide-lang.org/docs\n\nTo see some example code, look in the tutorials directory.\n\nIf you've acquired a full source distribution and want to build Halide, see the\n[notes below](#building-halide-with-cmake).\n\n# Getting Halide\n\n## Binary tarballs\n\nThe latest version of Halide is **Halide 13.0.2**. We provide binary releases\nfor many popular platforms and architectures, including 32/64-bit x86 Windows,\n64-bit macOS, and 32/64-bit x86/ARM Ubuntu Linux. See the releases tab on the\nright (or click [here](https://github.com/halide/Halide/releases/tag/v13.0.1)).\n\n## Vcpkg\n\nIf you use [vcpkg](https://github.com/microsoft/vcpkg) to manage dependencies,\nyou can install Halide via:\n\n```\n$ vcpkg install halide:x64-windows # or x64-linux/x64-osx\n```\n\nOne caveat: vcpkg installs only the minimum Halide backends required to compile\ncode for the active platform. If you want to include all the backends, you\nshould install `halide[target-all]:x64-windows` instead. Note that since this\nwill build LLVM, it will take a _lot_ of disk space (up to 100GB).\n\n## Homebrew\n\nAlternatively, if you use macOS, you can install Halide via\n[Homebrew](https://brew.sh/) like so:\n\n```\n$ brew install halide\n```\n\n## Other package managers\n\nWe are interested in bringing Halide 12 to other popular package managers and\nLinux distribution repositories including, but not limited to, Conan,\nDebian, [Ubuntu (or PPA)](https://github.com/halide/Halide/issues/5285),\nCentOS/Fedora, and Arch. If you have experience publishing packages we would be\nhappy to work with you!\n\nIf you are a maintainer of any other package distribution platform, we would be\nexcited to work with you, too.\n\n# Platform Support\n\nThere are two sets of platform requirements relevant to Halide: those required\nto run the compiler library in either JIT or AOT mode, and those required to run\nthe _binary outputs_ of the AOT compiler.\n\nThese are the **tested** host toolchain and platform combinations for building\nand running the Halide compiler library.\n\n| Compiler   | Version      | OS                     | Architectures   |\n| ---------- | ------------ | ---------------------- | --------------- |\n| GCC        | 7.5          | Ubuntu Linux 20.04 LTS | x86, x64, ARM32 |\n| GCC        | 7.5          | Ubuntu Linux 18.04 LTS | ARM32, ARM64    |\n| MSVC       | 2019 (19.28) | Windows 10 (20H2)      | x86, x64        |\n| AppleClang | 12.0.0       | macOS 10.15            | x86_64          |\n| AppleClang | 12.0.0       | macOS 11.1             | ARM64           |\n\nSome users have successfully built Halide for Linux using Clang 9.0.0+, for\nWindows using ClangCL 11.0.0+, and for Windows ARM64 by cross-compiling with\nMSVC. We do not actively test these scenarios, however, so your mileage may\nvary.\n\nBeyond these, we are willing to support (by accepting PRs for) platform and\ntoolchain combinations that still receive _active, first-party, public support_\nfrom their original vendors. For instance, at time of writing, this excludes\nWindows 7 and includes Ubuntu 18.04 LTS.\n\nCompiled AOT pipelines are expected to have much broader platform support. The\nbinaries use the C ABI, and we expect any compliant C compiler to be able to use\nthe generated headers correctly. The C++ bindings currently require C++17. If\nyou discover a compatibility problem with a generated pipeline, please open an\nissue.\n\n# Building Halide with Make\n\n### TL;DR\n\nHave llvm-11.0 (or greater) installed and run `make` in the root directory of\nthe repository (where this README is).\n\n### Acquiring LLVM\n\nAt any point in time, building Halide requires either the latest stable version\nof LLVM, the previous stable version of LLVM, and trunk. At the time of writing,\nthis means versions 12.0 and 11.0 are supported, but 10.0 is not. The commands\n`llvm-config` and `clang` must be somewhere in the path.\n\nIf your OS does not have packages for LLVM, you can find binaries for it at\nhttp://llvm.org/releases/download.html. Download an appropriate package and then\neither install it, or at least put the `bin` subdirectory in your path. (This\nworks well on OS X and Ubuntu.)\n\nIf you want to build it yourself, first check it out from GitHub:\n\n```\n% git clone --depth 1 --branch llvmorg-12.0.0 https://github.com/llvm/llvm-project.git\n```\n\n(If you want to build LLVM 11.x, use branch `llvmorg-11.1.0`; for current trunk,\nuse `main`)\n\nThen build it like so:\n\n```\n% cmake -DCMAKE_BUILD_TYPE=Release \\\n        -DLLVM_ENABLE_PROJECTS=\"clang;lld;clang-tools-extra\" \\\n        -DLLVM_TARGETS_TO_BUILD=\"X86;ARM;NVPTX;AArch64;Mips;Hexagon;WebAssembly\" \\\n        -DLLVM_ENABLE_TERMINFO=OFF -DLLVM_ENABLE_ASSERTIONS=ON \\\n        -DLLVM_ENABLE_EH=ON -DLLVM_ENABLE_RTTI=ON -DLLVM_BUILD_32_BITS=OFF \\\n        -S llvm-project/llvm -B llvm-build\n% cmake --build llvm-build\n% cmake --install llvm-build --prefix llvm-install\n```\n\nRunning a serial build will be slow. To improve speed, try running a parallel\nbuild. That's done by default in Ninja; for make, use the option -j NNN,\nwhere NNN is the number of parallel jobs, e.g. the number of CPUs you have.\nThen, point Halide to it:\n\n```\n% export LLVM_ROOT=$PWD/llvm-install\n% export LLVM_CONFIG=$LLVM_ROOT/bin/llvm-config\n```\n\nNote that you _must_ add `clang` to `LLVM_ENABLE_PROJECTS`; adding `lld` to\n`LLVM_ENABLE_PROJECTS` is only required when using WebAssembly, and adding\n`clang-tools-extra` is only necessary if you plan to contribute code to Halide\n(so that you can run `clang-tidy` on your pull requests). We recommend enabling\nboth in all cases to simplify builds. You can disable exception handling (EH)\nand RTTI if you don't want the Python bindings.\n\n### Building Halide with make\n\nWith `LLVM_CONFIG` set (or `llvm-config` in your path), you should be able to\njust run `make` in the root directory of the Halide source tree.\n`make run_tests` will run the JIT test suite, and `make test_apps` will make\nsure all the apps compile and run (but won't check their output).\n\nThere is no `make install`. If you want to make an install package, use CMake.\n\n### Building Halide out-of-tree with make\n\nIf you wish to build Halide in a separate directory, you can do that like so:\n\n    % cd ..\n    % mkdir halide_build\n    % cd halide_build\n    % make -f ../Halide/Makefile\n\n# Building Halide with CMake\n\n### MacOS and Linux\n\nFollow the above instructions to build LLVM or acquire a suitable binary\nrelease. Then change directory to the Halide repository and run:\n\n```\n% cmake -G Ninja -DCMAKE_BUILD_TYPE=Release -DLLVM_DIR=$LLVM_ROOT/lib/cmake/llvm -S . -B build\n% cmake --build build\n```\n\n`LLVM_DIR` is the folder in the LLVM installation tree **(do not use the build\ntree by mistake)** that contains `LLVMConfig.cmake`. It is not required to set\nthis variable if you have a suitable system-wide version installed. If you have\nmultiple system-wide versions installed, you can specify the version with\n`Halide_REQUIRE_LLVM_VERSION`. Remove `-G Ninja` if you prefer to build with a\ndifferent generator.\n\n### Windows\n\nWe suggest building with Visual Studio 2019. Your mileage may vary with earlier\nversions. Be sure to install the \"C++ CMake tools for Windows\" in the Visual\nStudio installer. For older versions of Visual Studio, do not install the CMake\ntools, but instead acquire CMake and Ninja from their respective project\nwebsites.\n\nThese instructions start from the `D:` drive. We assume this git repo is cloned\nto `D:\\Halide`. We also assume that your shell environment is set up correctly.\nFor a 64-bit build, run:\n\n```\nD:\\> \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Auxiliary\\Build\\vcvarsall.bat\" x64\n```\n\nFor a 32-bit build, run:\n\n```\nD:\\> \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Auxiliary\\Build\\vcvarsall.bat\" x64_x86\n```\n\n#### Managing dependencies with vcpkg\n\nThe best way to get compatible dependencies on Windows is to use\n[vcpkg](https://github.com/Microsoft/vcpkg). Install it like so:\n\n```\nD:\\> git clone https://github.com/Microsoft/vcpkg.git\nD:\\> cd vcpkg\nD:\\> .\\bootstrap-vcpkg.bat\nD:\\vcpkg> .\\vcpkg integrate install\n...\nCMake projects should use: \"-DCMAKE_TOOLCHAIN_FILE=D:/vcpkg/scripts/buildsystems/vcpkg.cmake\"\n```\n\nThen install the libraries. For a 64-bit build, run:\n\n```\nD:\\vcpkg> .\\vcpkg install libpng:x64-windows libjpeg-turbo:x64-windows llvm[target-all,clang-tools-extra]:x64-windows\n```\n\nTo support 32-bit builds, also run:\n\n```\nD:\\vcpkg> .\\vcpkg install libpng:x86-windows libjpeg-turbo:x86-windows llvm[target-all,clang-tools-extra]:x86-windows\n```\n\n#### Building Halide\n\nCreate a separate build tree and call CMake with vcpkg's toolchain. This will\nbuild in either 32-bit or 64-bit depending on the environment script (`vcvars`)\nthat was run earlier.\n\n```\nD:\\Halide> cmake -G Ninja ^\n                 -DCMAKE_BUILD_TYPE=Release ^\n                 -DCMAKE_TOOLCHAIN_FILE=D:/vcpkg/scripts/buildsystems/vcpkg.cmake ^\n                 -S . -B build\n```\n\n**Note:** If building with Python bindings on 32-bit (enabled by default), be\nsure to point CMake to the installation path of a 32-bit Python 3. You can do\nthis by specifying, for example:\n`\"-DPython3_ROOT_DIR=C:\\Program Files (x86)\\Python38-32\"`.\n\nThen run the build with:\n\n```\nD:\\Halide> cmake --build build --config Release\n```\n\nTo run all the tests:\n\n```\nD:\\Halide> cd build\nD:\\Halide\\build> ctest -C Release\n```\n\nSubsets of the tests can be selected with `-L` and include `correctness`,\n`python`, `error`, and the other directory names under `/tests`.\n\n#### Building LLVM (optional)\n\nFollow these steps if you want to build LLVM yourself. First, download LLVM's\nsources (these instructions use the latest 12.0 release)\n\n```\nD:\\> git clone --depth 1 --branch llvmorg-12.0.0 https://github.com/llvm/llvm-project.git\n```\n\nFor a 64-bit build, run:\n\n```\nD:\\> cmake -G Ninja ^\n           -DCMAKE_BUILD_TYPE=Release ^\n           -DLLVM_ENABLE_PROJECTS=clang;lld;clang-tools-extra ^\n           -DLLVM_ENABLE_TERMINFO=OFF ^\n           -DLLVM_TARGETS_TO_BUILD=X86;ARM;NVPTX;AArch64;Mips;Hexagon ^\n           -DLLVM_ENABLE_ASSERTIONS=ON ^\n           -DLLVM_ENABLE_EH=ON ^\n           -DLLVM_ENABLE_RTTI=ON ^\n           -DLLVM_BUILD_32_BITS=OFF ^\n           -S llvm-project\\llvm -B llvm-build\n```\n\nFor a 32-bit build, run:\n\n```\nD:\\> cmake -G Ninja ^\n           -DCMAKE_BUILD_TYPE=Release ^\n           -DLLVM_ENABLE_PROJECTS=clang;lld;clang-tools-extra ^\n           -DLLVM_ENABLE_TERMINFO=OFF ^\n           -DLLVM_TARGETS_TO_BUILD=X86;ARM;NVPTX;AArch64;Mips;Hexagon ^\n           -DLLVM_ENABLE_ASSERTIONS=ON ^\n           -DLLVM_ENABLE_EH=ON ^\n           -DLLVM_ENABLE_RTTI=ON ^\n           -DLLVM_BUILD_32_BITS=ON ^\n           -S llvm-project\\llvm -B llvm32-build\n```\n\nFinally, run:\n\n```\nD:\\> cmake --build llvm-build --config Release\nD:\\> cmake --install llvm-build --prefix llvm-install\n```\n\nYou can substitute `Debug` for `Release` in the above `cmake` commands if you\nwant a debug build. Make sure to add `-DLLVM_DIR=D:/llvm-install/lib/cmake/llvm`\nto the Halide CMake command to override `vcpkg`'s LLVM.\n\n**MSBuild:** If you want to build LLVM with MSBuild instead of Ninja, use\n`-G \"Visual Studio 16 2019\" -Thost=x64 -A x64` or\n`-G \"Visual Studio 16 2019\" -Thost=x64 -A Win32` in place of `-G Ninja`.\n\n#### If all else fails...\n\nDo what the build-bots do: https://buildbot.halide-lang.org/master/#/builders\n\nIf the column that best matches your system is red, then maybe things aren't\njust broken for you. If it's green, then you can click the \"stdio\" links in the\nlatest build to see what commands the build bots run, and what the output was.\n\n# Some useful environment variables\n\n`HL_TARGET=...` will set Halide's AOT compilation target.\n\n`HL_JIT_TARGET=...` will set Halide's JIT compilation target.\n\n`HL_DEBUG_CODEGEN=1` will print out pseudocode for what Halide is compiling.\nHigher numbers will print more detail.\n\n`HL_NUM_THREADS=...` specifies the number of threads to create for the thread\npool. When the async scheduling directive is used, more threads than this number\nmay be required and thus allocated. A maximum of 256 threads is allowed. (By\ndefault, the number of cores on the host is used.)\n\n`HL_TRACE_FILE=...` specifies a binary target file to dump tracing data into\n(ignored unless at least one `trace_` feature is enabled in `HL_TARGET` or\n`HL_JIT_TARGET`). The output can be parsed programmatically by starting from the\ncode in `utils/HalideTraceViz.cpp`.\n\n# Using Halide on OSX\n\nPrecompiled Halide distributions are built using XCode's command-line tools with\nApple clang 500.2.76. This means that we link against libc++ instead of\nlibstdc++. You may need to adjust compiler options accordingly if you're using\nan older XCode which does not default to libc++.\n\n# Halide for Hexagon HVX\n\nHalide supports offloading work to Qualcomm Hexagon DSP on Qualcomm Snapdragon\n845/710 devices or newer. The Hexagon DSP provides a set of 128 byte vector\ninstruction extensions - the Hexagon Vector eXtensions (HVX). HVX is well suited\nfor image processing, and Halide for Hexagon HVX will generate the appropriate\nHVX vector instructions from a program authored in Halide.\n\nHalide can be used to compile Hexagon object files directly, by using a target\nsuch as `hexagon-32-qurt-hvx`.\n\nHalide can also be used to offload parts of a pipeline to Hexagon using the\n`hexagon` scheduling directive. To enable the `hexagon` scheduling directive,\ninclude the `hvx` target feature in your target. The currently supported\ncombination of targets is to use the HVX target features with an x86 linux\nhost (to use the simulator) or with an ARM android target (to use Hexagon DSP\nhardware). For examples of using the `hexagon` scheduling directive on both the\nsimulator and a Hexagon DSP, see the blur example app.\n\nTo build and run an example app using the Hexagon target,\n\n1. Obtain and build trunk LLVM and Clang. (Earlier versions of LLVM may work but\n   are not actively tested and thus not recommended.)\n2. Download and install the Hexagon SDK and Hexagon Tools. Hexagon SDK 4.3.0 or\n   later is needed. Hexagon Tools 8.4 or later is needed.\n3. Build and run an example for Hexagon HVX\n\n### 1. Obtain and build trunk LLVM and Clang\n\n(Follow the instructions given previously, just be sure to check out the `main`\nbranch.)\n\n### 2. Download and install the Hexagon SDK and Hexagon Tools\n\nGo to https://developer.qualcomm.com/software/hexagon-dsp-sdk/tools\n\n1. Select the Hexagon Series 600 Software and download & run QPM and install\n   the Hexagon SDK 4.3.0 version or later for Linux.\n2. untar the installer\n3. Run the extracted installer to install the Hexagon SDK and Hexagon Tools,\n   selecting Installation of Hexagon SDK into `/location/of/SDK/Hexagon_SDK/4.x`\n   and the Hexagon tools into `/location/of/SDK/Hexagon_Tools/8.x`\n4. Set an environment variable to point to the SDK installation location\n   ```\n   export SDK_LOC=/location/of/SDK\n   ```\n\n### 3. Build and run an example for Hexagon HVX\n\nIn addition to running Hexagon code on device, Halide also supports running\nHexagon code on the simulator from the Hexagon tools.\n\nTo build and run the blur example in Halide/apps/blur on the simulator:\n\n```\ncd apps/blur\nexport HL_HEXAGON_SIM_REMOTE=../../src/runtime/hexagon_remote/bin/v65/hexagon_sim_remote\nexport HL_HEXAGON_TOOLS=$SDK_LOC/Hexagon_Tools/8.x/Tools/\nLD_LIBRARY_PATH=../../src/runtime/hexagon_remote/bin/host/:$HL_HEXAGON_TOOLS/lib/iss/:. HL_TARGET=host-hvx make test\n```\n\n### To build and run the blur example in Halide/apps/blur on Android:\n\nTo build the example for Android, first ensure that you have Android NDK r19b or\nlater installed, and the ANDROID_NDK_ROOT environment variable points to it.\n(Note that Qualcomm Hexagon SDK v4.3.0 includes Android NDK r19c, which is\nfine.)\n\nNow build and run the blur example using the script to run it on device:\n\n```\nexport HL_HEXAGON_TOOLS=$SDK_LOC/HEXAGON_Tools/8.4.11/Tools/\nHL_TARGET=arm-64-android-hvx ./adb_run_on_device.sh\n```\n"
}
{
    "primary_language": "Java",
    "language_guidelines": "# General Guidelines:\n**General Guidelines for Java Projects**\n\n1. **Read the README**  \n   Start by reading the project's README file on GitHub. It often contains important instructions for installation, usage, and any project-specific details.  \n\n2. **Check Dependencies**  \n   Look for any dependencies listed in the README or in configuration files like `pom.xml` (for Maven) or `build.gradle` (for Gradle). Ensure you have the required JDK version installed.  \n\n3. **Build Tool**  \n   Identify which build tool the project is using: Maven or Gradle. This information should be available in the README or through project configuration files (`pom.xml` for Maven, `build.gradle` for Gradle).  \n\n4. **Build the Project**  \n   Use the appropriate commands based on the build tool:  \n   - For Maven:  \n     ```  \n     mvn clean install  \n     ```  \n   - For Gradle:  \n     ```  \n     gradle build  \n     ```  \n\n5. **Configuration**  \n   Check if the project requires any configuration files (e.g., property files, YAML files) and set them up accordingly.  \n\n6. **Run Tests (if available)**  \n   If the project provides tests, it\u2019s a good idea to run them to ensure everything is working correctly.  \n\n7. **Run the Project**  \n   Follow the instructions in the README to run the project. This could involve running a specific class, starting a server, or executing a specific command.  \n\n8. **Troubleshooting**  \n   If you encounter any issues during installation or while running the project, refer to the project\u2019s issue tracker on GitHub or search for similar issues others may have encountered.  \n\n9. **Test Suite Results**  \n   When running a test suite, it is normal for some test cases to fail. If the percentage of failing test cases is less than 20% of the total number, it is considered acceptable, and further investigation is usually not necessary.  ",
    "workflow_guidelines": "\nThe following workflow files might contain information on how to set up the project and run test cases.  This might be useful later on when building/installing and testing the project:\n\nfile: build-public-image.yml\nContent:\nname: Build Docker image and push\non:\n  workflow_dispatch:\n  pull_request:\n    types: ['labeled']\njobs:\n  build:\n    if: ${{ github.event.label.name == 'status/image_testing' }}\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n        with:\n          ref: ${{ github.event.pull_request.head.sha }}\n      - name: get branch name\n        id: extract_branch\n        run: |\n          tag='${{ github.event.pull_request.number }}'\n          echo \"tag=${tag}\" >> $GITHUB_OUTPUT\n      - name: Set up JDK\n        uses: actions/setup-java@v3\n        with:\n          java-version: '17'\n          distribution: 'zulu'\n          cache: 'maven'\n      - name: Build\n        id: build\n        run: |\n          ./mvnw -B -ntp versions:set -DnewVersion=$GITHUB_SHA\n          ./mvnw -B -V -ntp clean package -Pprod -DskipTests\n          export VERSION=$(./mvnw -q -Dexec.executable=echo -Dexec.args='${project.version}' --non-recursive exec:exec)\n          echo \"version=${VERSION}\" >> $GITHUB_OUTPUT\n      - name: Set up QEMU\n        uses: docker/setup-qemu-action@v2\n      - name: Set up Docker Buildx\n        id: buildx\n        uses: docker/setup-buildx-action@v2\n      - name: Cache Docker layers\n        uses: actions/cache@v3\n        with:\n          path: /tmp/.buildx-cache\n          key: ${{ runner.os }}-buildx-${{ github.sha }}\n          restore-keys: |\n            ${{ runner.os }}-buildx-\n      - name: Configure AWS credentials for Kafka-UI account\n        uses: aws-actions/configure-aws-credentials@v1-node16\n        with:\n          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}\n          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n          aws-region: us-east-1\n      - name: Login to Amazon ECR\n        id: login-ecr\n        uses: aws-actions/amazon-ecr-login@v1\n        with:\n          registry-type: 'public'\n      - name: Build and push\n        id: docker_build_and_push\n        uses: docker/build-push-action@v4\n        with:\n          builder: ${{ steps.buildx.outputs.name }}\n          context: kafka-ui-api\n          push: true\n          tags: public.ecr.aws/provectus/kafka-ui-custom-build:${{ steps.extract_branch.outputs.tag }}\n          build-args: |\n            JAR_FILE=kafka-ui-api-${{ steps.build.outputs.version }}.jar\n          cache-from: type=local,src=/tmp/.buildx-cache\n          cache-to: type=local,dest=/tmp/.buildx-cache\n      - name: make comment with private deployment link\n        uses: peter-evans/create-or-update-comment@v2\n        with:\n          issue-number: ${{ github.event.pull_request.number }}\n          body: |\n            Image published at public.ecr.aws/provectus/kafka-ui-custom-build:${{ steps.extract_branch.outputs.tag }}\n\n    outputs:\n      tag: ${{ steps.extract_branch.outputs.tag }}\n\nfile: build-template.yml\nContent:\nname: Maven build template\non:\n  workflow_call:\n   inputs:\n    APP_VERSION:\n     required: true\n     type: string\njobs:\n  build:\n    runs-on: ubuntu-latest\n    outputs:\n      version: ${{steps.build.outputs.version}}\n    steps:\n      - uses: actions/checkout@v3\n        with:\n          ref: ${{ github.event.pull_request.head.sha }}\n      - run: |\n          git config user.name github-actions\n          git config user.email github-actions@github.com\n      - name: Set up JDK\n        uses: actions/setup-java@v3\n        with:\n          java-version: '17'\n          distribution: 'zulu'\n          cache: 'maven'\n      - name: Build\n        id: build\n        run: |\n          ./mvnw -B -ntp versions:set -DnewVersion=${{ inputs.APP_VERSION }}\n          ./mvnw -B -V -ntp clean package -Pprod -DskipTests\n          export VERSION=$(./mvnw -q -Dexec.executable=echo -Dexec.args='${project.version}' --non-recursive exec:exec)\n          echo \"version=${VERSION}\" >> $GITHUB_OUTPUT\n",
    "readme": "![UI for Apache Kafka logo](documentation/images/kafka-ui-logo.png) UI for Apache Kafka&nbsp;\n------------------\n#### Versatile, fast and lightweight web UI for managing Apache Kafka\u00ae clusters. Built by developers, for developers.\n\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/provectus/kafka-ui/blob/master/LICENSE)\n![UI for Apache Kafka Price Free](documentation/images/free-open-source.svg)\n[![Release version](https://img.shields.io/github/v/release/provectus/kafka-ui)](https://github.com/provectus/kafka-ui/releases)\n[![Chat with us](https://img.shields.io/discord/897805035122077716)](https://discord.gg/4DWzD7pGE5)\n\n### DISCLAIMER\n<em>UI for Apache Kafka is a free tool built and supported by the open-source community. Curated by Provectus, it will remain free and open-source, without any paid features or subscription plans to be added in the future.\nLooking for the help of Kafka experts? Provectus can help you design, build, deploy, and manage Apache Kafka clusters and streaming applications. Discover [Professional Services for Apache Kafka](https://provectus.com/professional-services-apache-kafka/), to unlock the full potential of Kafka in your enterprise! </em>\n\n\n#### UI for Apache Kafka is a free, open-source web UI to monitor and manage Apache Kafka clusters.\n\nUI for Apache Kafka is a simple tool that makes your data flows observable, helps find and troubleshoot issues faster and deliver optimal performance. Its lightweight dashboard makes it easy to track key metrics of your Kafka clusters - Brokers, Topics, Partitions, Production, and Consumption.\n\nSet up UI for Apache Kafka with just a couple of easy commands to visualize your Kafka data in a comprehensible way. You can run the tool locally or in\nthe cloud.\n\n![Interface](documentation/images/Interface.gif)\n\n# Features\n* **Multi-Cluster Management** \u2014 monitor and manage all your clusters in one place\n* **Performance Monitoring with Metrics Dashboard** \u2014  track key Kafka metrics with a lightweight dashboard\n* **View Kafka Brokers** \u2014 view topic and partition assignments, controller status\n* **View Kafka Topics** \u2014 view partition count, replication status, and custom configuration\n* **View Consumer Groups** \u2014 view per-partition parked offsets, combined and per-partition lag\n* **Browse Messages** \u2014 browse messages with JSON, plain text, and Avro encoding\n* **Dynamic Topic Configuration** \u2014 create and configure new topics with dynamic configuration\n* **Configurable Authentification** \u2014 secure your installation with optional Github/Gitlab/Google OAuth 2.0\n* **Custom serialization/deserialization plugins** - use a ready-to-go serde for your data like AWS Glue or Smile, or code your own!\n* **Role based access control** - [manage permissions](https://github.com/provectus/kafka-ui/wiki/RBAC-(role-based-access-control)) to access the UI with granular precision\n* **Data masking** - [obfuscate](https://github.com/provectus/kafka-ui/blob/master/documentation/guides/DataMasking.md) sensitive data in topic messages\n\n# The Interface\nUI for Apache Kafka wraps major functions of Apache Kafka with an intuitive user interface.\n\n![Interface](documentation/images/Interface.gif)\n\n## Topics\nUI for Apache Kafka makes it easy for you to create topics in your browser by several clicks,\npasting your own parameters, and viewing topics in the list.\n\n![Create Topic](documentation/images/Create_topic_kafka-ui.gif)\n\nIt's possible to jump from connectors view to corresponding topics and from a topic to consumers (back and forth) for more convenient navigation.\nconnectors, overview topic settings.\n\n![Connector_Topic_Consumer](documentation/images/Connector_Topic_Consumer.gif)\n\n### Messages\nLet's say we want to produce messages for our topic. With the UI for Apache Kafka we can send or write data/messages to the Kafka topics without effort by specifying parameters, and viewing messages in the list.\n\n![Produce Message](documentation/images/Create_message_kafka-ui.gif)\n\n## Schema registry\nThere are 3 supported types of schemas: Avro\u00ae, JSON Schema, and Protobuf schemas.\n\n![Create Schema Registry](documentation/images/Create_schema.gif)\n\nBefore producing avro-encoded messages, you have to add an avro schema for the topic in Schema Registry. Now all these steps are easy to do\nwith a few clicks in a user-friendly interface.\n\n![Avro Schema Topic](documentation/images/Schema_Topic.gif)\n\n# Getting Started\n\nTo run UI for Apache Kafka, you can use a pre-built Docker image or build it locally.\n\n## Configuration\n\nWe have plenty of [docker-compose files](documentation/compose/DOCKER_COMPOSE.md) as examples. They're built for various configuration stacks.\n\n# Guides\n\n- [SSO configuration](documentation/guides/SSO.md)\n- [AWS IAM configuration](documentation/guides/AWS_IAM.md)\n- [Docker-compose files](documentation/compose/DOCKER_COMPOSE.md)\n- [Connection to a secure broker](documentation/guides/SECURE_BROKER.md)\n- [Configure seriliazation/deserialization plugins or code your own](documentation/guides/Serialization.md)\n\n### Configuration File\nExample of how to configure clusters in the [application-local.yml](https://github.com/provectus/kafka-ui/blob/master/kafka-ui-api/src/main/resources/application-local.yml) configuration file:\n\n\n```sh\nkafka:\n  clusters:\n    -\n      name: local\n      bootstrapServers: localhost:29091\n      schemaRegistry: http://localhost:8085\n      schemaRegistryAuth:\n        username: username\n        password: password\n#     schemaNameTemplate: \"%s-value\"\n      metrics:\n        port: 9997\n        type: JMX\n    -\n```\n\n* `name`: cluster name\n* `bootstrapServers`: where to connect\n* `schemaRegistry`: schemaRegistry's address\n* `schemaRegistryAuth.username`: schemaRegistry's basic authentication username\n* `schemaRegistryAuth.password`: schemaRegistry's basic authentication password\n* `schemaNameTemplate`: how keys are saved to schemaRegistry\n* `metrics.port`: open JMX port of a broker\n* `metrics.type`: Type of metrics, either JMX or PROMETHEUS. Defaulted to JMX.\n* `readOnly`: enable read only mode\n\nConfigure as many clusters as you need by adding their configs below separated with `-`.\n\n## Running a Docker Image\nThe official Docker image for UI for Apache Kafka is hosted here: [hub.docker.com/r/provectuslabs/kafka-ui](https://hub.docker.com/r/provectuslabs/kafka-ui).\n\nLaunch Docker container in the background:\n```sh\n\ndocker run -p 8080:8080 \\\n\t-e KAFKA_CLUSTERS_0_NAME=local \\\n\t-e KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka:9092 \\\n\t-d provectuslabs/kafka-ui:latest\n\n```\nThen access the web UI at [http://localhost:8080](http://localhost:8080).\nFurther configuration with environment variables - [see environment variables](#env_variables)\n\n### Docker Compose\n\nIf you prefer to use `docker-compose` please refer to the [documentation](docker-compose.md).\n\n### Helm chart\nHelm chart could be found under [charts/kafka-ui](https://github.com/provectus/kafka-ui/tree/master/charts/kafka-ui) directory\n\nQuick-start instruction [here](helm_chart.md)\n\n## Building With Docker\n\n### Prerequisites\n\nCheck [prerequisites.md](documentation/project/contributing/prerequisites.md)\n\n### Building and Running\n\nCheck [building.md](documentation/project/contributing/building.md)\n\n## Building Without Docker\n\n### Prerequisites\n\n[Prerequisites](documentation/project/contributing/prerequisites.md) will mostly remain the same with the exception of docker.\n\n### Running without Building\n\n[How to run quickly without building](documentation/project/contributing/building-and-running-without-docker.md#run_without_docker_quickly)\n\n### Building and Running\n\n[How to build and run](documentation/project/contributing/building-and-running-without-docker.md#build_and_run_without_docker)\n\n## Liveliness and readiness probes\nLiveliness and readiness endpoint is at `/actuator/health`.\nInfo endpoint (build info) is located at `/actuator/info`.\n\n## <a name=\"env_variables\"></a> Environment Variables\n\nAlternatively, each variable of the .yml file can be set with an environment variable.\nFor example, if you want to use an environment variable to set the `name` parameter, you can write it like this: `KAFKA_CLUSTERS_2_NAME`\n\n|Name               \t|Description\n|-----------------------|-------------------------------\n|`SERVER_SERVLET_CONTEXT_PATH` | URI basePath\n|`LOGGING_LEVEL_ROOT`        \t| Setting log level (trace, debug, info, warn, error). Default: info\n|`LOGGING_LEVEL_COM_PROVECTUS` |Setting log level (trace, debug, info, warn, error). Default: debug\n|`SERVER_PORT` |Port for the embedded server. Default: `8080`\n|`KAFKA_ADMIN-CLIENT-TIMEOUT` | Kafka API timeout in ms. Default: `30000`\n|`KAFKA_CLUSTERS_0_NAME` | Cluster name\n|`KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS` \t|Address where to connect\n|`KAFKA_CLUSTERS_0_KSQLDBSERVER` \t| KSQL DB server address\n|`KAFKA_CLUSTERS_0_KSQLDBSERVERAUTH_USERNAME` \t| KSQL DB server's basic authentication username\n|`KAFKA_CLUSTERS_0_KSQLDBSERVERAUTH_PASSWORD` \t| KSQL DB server's basic authentication password\n|`KAFKA_CLUSTERS_0_KSQLDBSERVERSSL_KEYSTORELOCATION`   \t|Path to the JKS keystore to communicate to KSQL DB\n|`KAFKA_CLUSTERS_0_KSQLDBSERVERSSL_KEYSTOREPASSWORD`   \t|Password of the JKS keystore for KSQL DB\n|`KAFKA_CLUSTERS_0_PROPERTIES_SECURITY_PROTOCOL` \t|Security protocol to connect to the brokers. For SSL connection use \"SSL\", for plaintext connection don't set this environment variable\n|`KAFKA_CLUSTERS_0_SCHEMAREGISTRY`   \t|SchemaRegistry's address\n|`KAFKA_CLUSTERS_0_SCHEMAREGISTRYAUTH_USERNAME`   \t|SchemaRegistry's basic authentication username\n|`KAFKA_CLUSTERS_0_SCHEMAREGISTRYAUTH_PASSWORD`   \t|SchemaRegistry's basic authentication password\n|`KAFKA_CLUSTERS_0_SCHEMAREGISTRYSSL_KEYSTORELOCATION`   \t|Path to the JKS keystore to communicate to SchemaRegistry\n|`KAFKA_CLUSTERS_0_SCHEMAREGISTRYSSL_KEYSTOREPASSWORD`   \t|Password of the JKS keystore for SchemaRegistry\n|`KAFKA_CLUSTERS_0_METRICS_SSL`          |Enable SSL for Metrics (for PROMETHEUS metrics type). Default: false.\n|`KAFKA_CLUSTERS_0_METRICS_USERNAME` |Username for Metrics authentication\n|`KAFKA_CLUSTERS_0_METRICS_PASSWORD` |Password for Metrics authentication\n|`KAFKA_CLUSTERS_0_METRICS_KEYSTORELOCATION` |Path to the JKS keystore to communicate to metrics source (JMX/PROMETHEUS). For advanced setup, see `kafka-ui-jmx-secured.yml`\n|`KAFKA_CLUSTERS_0_METRICS_KEYSTOREPASSWORD` |Password of the JKS metrics keystore\n|`KAFKA_CLUSTERS_0_SCHEMANAMETEMPLATE` |How keys are saved to schemaRegistry\n|`KAFKA_CLUSTERS_0_METRICS_PORT`        \t |Open metrics port of a broker\n|`KAFKA_CLUSTERS_0_METRICS_TYPE`        \t |Type of metrics retriever to use. Valid values are JMX (default) or PROMETHEUS. If Prometheus, then metrics are read from prometheus-jmx-exporter instead of jmx\n|`KAFKA_CLUSTERS_0_READONLY`        \t|Enable read-only mode. Default: false\n|`KAFKA_CLUSTERS_0_KAFKACONNECT_0_NAME` |Given name for the Kafka Connect cluster\n|`KAFKA_CLUSTERS_0_KAFKACONNECT_0_ADDRESS` |Address of the Kafka Connect service endpoint\n|`KAFKA_CLUSTERS_0_KAFKACONNECT_0_USERNAME`| Kafka Connect cluster's basic authentication username\n|`KAFKA_CLUSTERS_0_KAFKACONNECT_0_PASSWORD`| Kafka Connect cluster's basic authentication password\n|`KAFKA_CLUSTERS_0_KAFKACONNECT_0_KEYSTORELOCATION`| Path to the JKS keystore to communicate to Kafka Connect\n|`KAFKA_CLUSTERS_0_KAFKACONNECT_0_KEYSTOREPASSWORD`| Password of the JKS keystore for Kafka Connect\n|`KAFKA_CLUSTERS_0_POLLING_THROTTLE_RATE` |Max traffic rate (bytes/sec) that kafka-ui allowed to reach when polling messages from the cluster. Default: 0 (not limited)\n|`KAFKA_CLUSTERS_0_SSL_TRUSTSTORELOCATION`| Path to the JKS truststore to communicate to Kafka Connect, SchemaRegistry, KSQL, Metrics\n|`KAFKA_CLUSTERS_0_SSL_TRUSTSTOREPASSWORD`| Password of the JKS truststore for Kafka Connect, SchemaRegistry, KSQL, Metrics\n|`TOPIC_RECREATE_DELAY_SECONDS` |Time delay between topic deletion and topic creation attempts for topic recreate functionality. Default: 1\n|`TOPIC_RECREATE_MAXRETRIES`  |Number of attempts of topic creation after topic deletion for topic recreate functionality. Default: 15\n|`DYNAMIC_CONFIG_ENABLED`|Allow to change application config in runtime. Default: false.\n"
}
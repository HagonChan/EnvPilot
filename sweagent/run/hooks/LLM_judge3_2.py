import os
import json
import time
from openai import OpenAI
from tqdm import tqdm
from typing import List, Dict, Any, Optional
import concurrent.futures
from sweagent.utils.log import get_logger

logger = get_logger("LLM-as-Judge", emoji="ğŸ‘¨â€âš–ï¸")


class Config:
    """é…ç½®ç±»ï¼Œç®¡ç†æ‰€æœ‰é…ç½®ä¿¡æ¯"""

    def __init__(
        self,
        api_base: str = "https://ark.cn-beijing.volces.com/api/v3",
        api_key: str = "e1e81de4-a291-4aee-8c8b-d97f42f40969",
        model: str = "deepseek-v3-250324",
        max_retries: int = 3,
        max_workers: int = 8,
        temperature: float = 0.0,
    ):
        self.api_key = api_key
        self.api_base = api_base
        self.model = model
        self.max_retries = max_retries
        self.max_workers = max_workers
        self.temperature = temperature

        self.prompt = """
# Agent Environment Configuration Evaluation Task

## Background
I have designed an Agent for automatically configuring the runtime environment of GitHub repositories.

## Success Criteria 
Environment configuration is considered **successful** when **BOTH** conditions are met:

### Condition 1: Test Suite Execution
- **At least one complete test suite** can start, execute, and finish successfully;Examples: unit tests, integration tests, functional tests, etc.
- The testing framework must generate a complete test report structure
- Individual test case failures are acceptable as long as the suite completes

### Condition 2: Core Functionality Verification  
- **Main functional modules** can be imported and used normally
- Basic API calls return expected results
- Core features demonstrated through example code execution

## Failure Conditions
- Testing framework cannot start or execute normally
- Core modules cannot be imported or are fundamentally broken
- Agent exceeds maximum call limit without achieving basic functionality
- No test suite can complete execution

## Important Notes
- Even if 100 test cases fail, if the test suite completes and core functions work, consider it SUCCESS
- Focus on **framework operability** rather than test pass rates
- Distinguish between critical errors (import failures) and non-critical errors (specific test failures)

## Task Requirements
Based on the agent output trajectory I provide, judge whether the environment configuration meets the success criteria above.

## Output Format
1. First line: PASSED or FAILED (direct output, no other text)
2. Following lines: Detailed reasoning explaining your judgment based on the two criteria above
"""


class RuleEvaluator:
    """è§„åˆ™è¯„ä¼°å™¨ï¼Œè´Ÿè´£åŸºäºè§„åˆ™çš„è¯„ä¼°"""

    def __init__(self, config: Config):
        self.config = config

    def evaluate_by_rule(self, info: Dict[str, Any]) -> Optional[str]:
        """åŸºäºè§„åˆ™è¯„ä¼°ï¼Œè¿”å› None è¡¨ç¤ºéœ€è¦LLMè¯„ä¼°ï¼Œè¿”å›å­—ç¬¦ä¸²è¡¨ç¤ºè§„åˆ™å·²ç¡®å®šç»“æœ"""
        if not info:
            return None

        exit_status = info.get("exit_status", "")

        # æ£€æŸ¥æ˜¯å¦åŒ¹é…å¤±è´¥è§„åˆ™
        if exit_status in ["submitted (exit_rounds)", "exit_error", "", None]:
            return f"FAILED\nè§„åˆ™åˆ¤å®šå¤±è´¥: exit_status = {exit_status}"

        return None


class TrajectoryProcessor:
    """è½¨è¿¹æ–‡ä»¶å¤„ç†å™¨ï¼Œè´Ÿè´£æ–‡ä»¶è¯»å–å’Œå†…å®¹å¤„ç†"""

    def __init__(self, config: Config):
        self.config = config

    def find_traj_files(self, root_dir: str) -> List[str]:
        """æŸ¥æ‰¾æ‰€æœ‰è½¨è¿¹æ–‡ä»¶"""
        traj_files = []
        for root, dirs, files in os.walk(root_dir):
            for file in files:
                if file.endswith(".traj"):
                    traj_files.append(os.path.join(root, file))
        return traj_files

    def _read_json_file(self, file_path: str) -> Dict[str, Any]:
        """è¯»å–JSONæ–‡ä»¶çš„é€šç”¨æ–¹æ³•"""
        try:
            with open(file_path, "r", encoding="utf-8") as file:
                return json.load(file)
        except Exception as e:
            logger.error(f"è¯»å–æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
            return {}

    def extract_history(self, file_path: str) -> List[Any]:
        """ä»è½¨è¿¹æ–‡ä»¶ä¸­æå–å†å²è®°å½•"""
        data = self._read_json_file(file_path)
        return data.get("history", [])

    def extract_info(self, file_path: str) -> Dict[str, Any]:
        """ä»è½¨è¿¹æ–‡ä»¶ä¸­æå–infoå­—æ®µ"""
        data = self._read_json_file(file_path)
        return data.get("info", {})


class LLMJudge:
    """LLMåˆ¤æ–­å™¨ï¼Œè´Ÿè´£è°ƒç”¨APIè¿›è¡Œè¯„ä¼°"""

    def __init__(self, config: Config):
        self.config = config
        self.client = OpenAI(api_key=config.api_key, base_url=config.api_base)

    def evaluate(self, processed_history: str) -> str:
        """è°ƒç”¨LLMè¿›è¡Œè¯„ä¼°"""
        for attempt in range(self.config.max_retries):
            try:
                response = self.client.chat.completions.create(
                    model=self.config.model,
                    messages=[
                        {"role": "system", "content": self.config.prompt},
                        {"role": "user", "content": processed_history},
                    ],
                    stream=False,
                    temperature=self.config.temperature,
                )
                return response.choices[0].message.content

            except Exception as e:
                error_msg = str(e)
                logger.warning(f"APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{self.config.max_retries}): {error_msg}")

                # å¤„ç†ä¸åŒç±»å‹çš„é”™è¯¯
                if "maximum context length" in error_msg:
                    processed_history = processed_history[-50000:]  # å¦‚æœè¿˜æ˜¯å¤ªé•¿ï¼Œè¿›ä¸€æ­¥æˆªæ–­
                    logger.info("æ£€æµ‹åˆ°ä¸Šä¸‹æ–‡è¿‡é•¿ï¼Œè¿›ä¸€æ­¥æˆªæ–­...")
                elif "400" in error_msg:
                    logger.warning("å¯èƒ½åŸå› : è¯·æ±‚æ ¼å¼é”™è¯¯æˆ–å†…å®¹è¿‡é•¿")
                elif "429" in error_msg:
                    logger.warning("å¯èƒ½åŸå› : APIè°ƒç”¨é¢‘ç‡è¿‡é«˜")
                    time.sleep(10)

                if attempt < self.config.max_retries - 1:
                    time.sleep(2**attempt)  # æŒ‡æ•°é€€é¿
                else:
                    return f"FAILED\næ— æ³•è°ƒç”¨APIï¼Œä¸Šä¸‹æ–‡å¯èƒ½è¿‡é•¿: {error_msg}"


class ReportManager:
    """æŠ¥å‘Šç®¡ç†å™¨ï¼Œè´Ÿè´£æŠ¥å‘Šçš„ç”Ÿæˆå’Œç®¡ç†"""

    def __init__(self, config: Config):
        self.config = config

    def load_cached_results(self, folder_name: str) -> Dict[str, List[str]]:
        """åŠ è½½ç¼“å­˜çš„ç»“æœ"""
        cached_results = {"passed": [], "failed": []}

        if not os.path.exists(folder_name):
            return cached_results

        report_file = os.path.join(folder_name, "report.json")
        if os.path.exists(report_file):
            try:
                with open(report_file, "r", encoding="utf-8") as file:
                    cached_results = json.load(file)

                # ç¡®ä¿å¿…è¦çš„é”®å­˜åœ¨
                for key in ["passed", "failed"]:
                    if key not in cached_results:
                        cached_results[key] = []

            except Exception as e:
                logger.error(f"è¯»å–ç¼“å­˜æ–‡ä»¶å‡ºé”™: {e}")

        return cached_results

    def is_already_evaluated(self, folder_name: str, pr_name: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²ç»è¯„ä¼°è¿‡"""
        txt_file = os.path.join(folder_name, f"{pr_name}.txt")
        return os.path.exists(txt_file)

    def save_result_to_file(self, folder_name: str, pr_name: str, result: str):
        """ä¿å­˜å•ä¸ªç»“æœåˆ°æ–‡ä»¶"""
        with open(os.path.join(folder_name, f"{pr_name}.txt"), "w", encoding="utf-8") as file:
            json.dump(result, file, ensure_ascii=False, indent=4)

    def save_report(self, folder_name: str, check_report: Dict[str, Any]):
        """ä¿å­˜æœ€ç»ˆæŠ¥å‘Š"""
        check_report["TOTAL_COUNT"] = {
            "passed_count": len(check_report["passed"]),
            "failed_count": len(check_report["failed"]),
            "total_count": len(check_report["passed"]) + len(check_report["failed"]),
        }
        # å¯¹åˆ—è¡¨è¿›è¡Œæ’åº
        check_report["passed"] = sorted(check_report["passed"])
        check_report["failed"] = sorted(check_report["failed"])

        with open(os.path.join(folder_name, "report.json"), "w", encoding="utf-8") as file:
            json.dump(check_report, file, ensure_ascii=False, indent=4)

    def generate_report_json(self, folder_name: str, pr_names: List[str]) -> Dict[str, Any]:
        """ç”Ÿæˆreport.json"""
        logger.info("=== å¼€å§‹ç”Ÿæˆreport.json ===")

        check_report = {
            "passed": [],
            "failed": [],
            "TOTAL_COUNT": {"passed_count": 0, "failed_count": 0, "total_count": 0},
        }

        with tqdm(pr_names, desc="ç”ŸæˆæŠ¥å‘Š", unit="item") as pbar:
            for pr_name in pbar:
                txt_file = os.path.join(folder_name, f"{pr_name}.txt")

                if os.path.exists(txt_file):
                    try:
                        with open(txt_file, "r", encoding="utf-8") as file:
                            result = json.load(file)

                        if result.startswith("PASSED"):
                            check_report["passed"].append(pr_name)
                        else:
                            check_report["failed"].append(pr_name)

                    except Exception as e:
                        check_report["failed"].append(pr_name)
                        pbar.set_postfix({"å½“å‰": pr_name, "çŠ¶æ€": "è¯»å–é”™è¯¯"})
                else:
                    check_report["failed"].append(pr_name)
                    pbar.set_postfix({"å½“å‰": pr_name, "çŠ¶æ€": "æ–‡ä»¶ç¼ºå¤±"})

        self.save_report(folder_name, check_report)

        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        total_count = check_report["TOTAL_COUNT"]
        logger.info(f"PASSED: {total_count.get('passed_count', 0)}")
        logger.info(f"FAILED: {total_count.get('failed_count', 0)}")
        logger.info(f"æ€»è®¡: {total_count.get('total_count', 0)}")
        logger.info("=== report.jsonç”Ÿæˆå®Œæˆ ===")

        return check_report


class EvaluationManager:
    """è¯„ä¼°ç®¡ç†å™¨ï¼Œåè°ƒå„ä¸ªç»„ä»¶å®Œæˆæ•´ä½“è¯„ä¼°æµç¨‹"""

    def __init__(self, config: Config):
        self.config = config
        self.trajectory_processor = TrajectoryProcessor(config)
        self.llm_judge = LLMJudge(config)
        self.report_manager = ReportManager(config)
        self.rule_evaluator = RuleEvaluator(config)

    def rule_evaluation_phase(self, root_dir: str, output_dir: str, pr_names: List[str]) -> List[str]:
        """è§„åˆ™è¯„ä¼°é˜¶æ®µï¼Œè¿”å›éœ€è¦LLMè¯„ä¼°çš„pr_names"""
        logger.info("=== å¼€å§‹è§„åˆ™è¯„ä¼°é˜¶æ®µ ===")

        rule_failed_count = 0
        rule_failed_list = []
        need_llm_evaluation = []

        with tqdm(pr_names, desc="è§„åˆ™è¯„ä¼°è¿›åº¦", unit="item") as pbar:
            for pr_name in pbar:
                # å¦‚æœå·²ç»è¯„ä¼°è¿‡ï¼Œè·³è¿‡
                if self.report_manager.is_already_evaluated(output_dir, pr_name):
                    need_llm_evaluation.append(pr_name)
                    continue

                try:
                    traj_file = os.path.join(root_dir, pr_name, f"{pr_name}.traj")
                    info = self.trajectory_processor.extract_info(traj_file)

                    rule_result = self.rule_evaluator.evaluate_by_rule(info)

                    if rule_result is not None:
                        # è§„åˆ™åˆ¤å®šä¸ºå¤±è´¥ï¼Œç›´æ¥ä¿å­˜ç»“æœ
                        self.report_manager.save_result_to_file(output_dir, pr_name, rule_result)
                        rule_failed_count += 1
                        rule_failed_list.append(pr_name)
                        pbar.set_postfix({"å½“å‰": pr_name, "çŠ¶æ€": "è§„åˆ™å¤±è´¥"})
                    else:
                        # éœ€è¦LLMè¯„ä¼°
                        need_llm_evaluation.append(pr_name)
                        pbar.set_postfix({"å½“å‰": pr_name, "çŠ¶æ€": "éœ€è¦LLM"})

                except Exception as e:
                    logger.error(f"è§„åˆ™è¯„ä¼° {pr_name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                    need_llm_evaluation.append(pr_name)
                    pbar.set_postfix({"å½“å‰": pr_name, "çŠ¶æ€": "é”™è¯¯"})

        logger.info(f"è§„åˆ™è¯„ä¼°å®Œæˆ:")
        logger.info(f"  è§„åˆ™åˆ¤å®šå¤±è´¥: {rule_failed_count} ä¸ª")
        logger.info(f"  éœ€è¦LLMè¯„ä¼°: {len(need_llm_evaluation)} ä¸ª")
        logger.info(f"è§„åˆ™åˆ¤å®šå¤±è´¥çš„é¡¹ç›®: {rule_failed_list}")
        logger.info("=== è§„åˆ™è¯„ä¼°é˜¶æ®µå®Œæˆ ===")

        return need_llm_evaluation

    def generate_llm_judge_result(self, root_dir: str, pr_name: str, output_dir: str) -> str:
        """å¤„ç†å•ä¸ªè½¨è¿¹æ–‡ä»¶å¹¶ç”Ÿæˆè¯„ä¼°ç»“æœ"""
        try:
            traj_file = os.path.join(root_dir, pr_name, f"{pr_name}.traj")
            history = self.trajectory_processor.extract_history(traj_file)

            if history:
                processed_history = str(history)
                final_status = self.llm_judge.evaluate(processed_history)
                self.report_manager.save_result_to_file(output_dir, pr_name, final_status)
                logger.debug(f"LLMå¤„ç†å®Œæˆ: {pr_name} - {final_status}")
                return final_status
            else:
                # æ²¡æœ‰historyæ—¶åˆ›å»ºå¤±è´¥è®°å½•
                # result = "FAILED\næœªæ‰¾åˆ°historyå­—æ®µ"
                # self.report_manager.save_result_to_file(output_dir, pr_name, result)
                logger.warning(f"æœªæ‰¾åˆ°history: {pr_name}")
                return "FAILED"

        except Exception as e:
            # åˆ›å»ºé”™è¯¯è®°å½•æ–‡ä»¶
            result = f"FAILED\nå¤„ç†é”™è¯¯: {str(e)}"
            # self.report_manager.save_result_to_file(output_dir, pr_name, result)
            logger.error(f"é”™è¯¯è¯¦æƒ… [{pr_name}]: {str(e)}")
            return "FAILED"

    def generate_txt_files(self, root_dir: str, output_dir: str, pr_names: List[str]):
        """ç”Ÿæˆtxtæ–‡ä»¶"""
        logger.info("=== å¼€å§‹ç”Ÿæˆtxtæ–‡ä»¶ ===")

        # å…ˆè¿›è¡Œè§„åˆ™è¯„ä¼°
        need_llm_evaluation = self.rule_evaluation_phase(root_dir, output_dir, pr_names)

        # è¿‡æ»¤å‡ºçœŸæ­£éœ€è¦LLMè¯„ä¼°çš„é¡¹ç›®ï¼ˆæ’é™¤å·²ç»è¯„ä¼°è¿‡çš„ï¼‰
        pending_items = [
            pr_name
            for pr_name in need_llm_evaluation
            if not self.report_manager.is_already_evaluated(output_dir, pr_name)
        ]

        if not pending_items:
            logger.info("æ²¡æœ‰å¾…å¤„ç†çš„LLMè¯„ä¼°é¡¹ç›®ï¼Œè·³è¿‡LLMè¯„ä¼°æ­¥éª¤")
            return

        logger.info(f"=== å¼€å§‹LLMè¯„ä¼°é˜¶æ®µ ===")
        logger.info(f"LLMè¯„ä¼°é¡¹ç›®æ•°: {len(pending_items)}")

        with concurrent.futures.ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
            futures = {
                executor.submit(self.generate_llm_judge_result, root_dir, pr_name, output_dir): pr_name
                for pr_name in pending_items
            }

            # ä½¿ç”¨é”æ¥ä¿è¯è¿›åº¦æ¡æ›´æ–°çš„åŸå­æ€§
            import threading

            update_lock = threading.Lock()

            # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
            with tqdm(
                total=len(pending_items),
                desc="LLMè¯„ä¼°è¿›åº¦",
                unit="item",
                miniters=1,
                mininterval=1.0,  # å¢åŠ æœ€å°æ—¶é—´é—´éš”åˆ°1ç§’
                leave=True,
            ) as pbar:
                completed_count = 0
                for future in concurrent.futures.as_completed(futures):
                    pr_name = futures[future]
                    try:
                        status = future.result()

                        # ä½¿ç”¨é”ä¿è¯æ›´æ–°æ“ä½œçš„åŸå­æ€§
                        with update_lock:
                            completed_count += 1
                            # åªåœ¨å®Œæˆä»»åŠ¡æ—¶æ›´æ–°ä¸€æ¬¡ï¼Œç®€åŒ–æ˜¾ç¤ºå†…å®¹
                            result_preview = "PASSED" if status.startswith("PASSED") else "FAILED"
                            pbar.set_postfix_str(
                                f"å®Œæˆ: {completed_count}/{len(pending_items)} | æœ€æ–°: {result_preview}"
                            )
                            pbar.update(1)

                    except Exception as e:
                        logger.error(f"å¤„ç† {pr_name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                        with update_lock:
                            completed_count += 1
                            pbar.set_postfix_str(f"å®Œæˆ: {completed_count}/{len(pending_items)} | æœ€æ–°: ERROR")
                            pbar.update(1)

        logger.info("=== LLMè¯„ä¼°é˜¶æ®µå®Œæˆ ===")
        logger.info("=== txtæ–‡ä»¶ç”Ÿæˆå®Œæˆ ===")

    def run_evaluation(self, root_dir: str, raw_dataset_file: str):
        """è¿è¡Œå®Œæ•´çš„è¯„ä¼°æµç¨‹"""
        output_dir = os.path.join(root_dir, "llm_judge")
        os.makedirs(output_dir, exist_ok=True)

        # è¯»å–æ•°æ®æ–‡ä»¶
        with open(raw_dataset_file, "r") as f:
            data = [json.loads(line) for line in f]

        pr_names = [f"{item['org']}__{item['repo']}-{item['number']}" for item in data]

        # ç”Ÿæˆtxtæ–‡ä»¶ï¼ˆåŒ…å«è§„åˆ™è¯„ä¼°å’ŒLLMè¯„ä¼°ï¼‰
        self.generate_txt_files(root_dir, output_dir, pr_names)

        # ç”ŸæˆæŠ¥å‘Š
        self.report_manager.generate_report_json(output_dir, pr_names)


def main():
    """ä¸»å‡½æ•°"""
    # åˆå§‹åŒ–é…ç½®
    config = Config()

    # åˆ›å»ºè¯„ä¼°ç®¡ç†å™¨
    evaluator = EvaluationManager(config)

    # è¿è¡Œè¯„ä¼°
    root_dir = "trajectories_PrepareScript/deepseek-v3_memory"
    data_file = "data/stage3_2_new_simplified.jsonl"

    evaluator.run_evaluation(root_dir, data_file)


if __name__ == "__main__":
    main()

##### LLM INPUT #####
================================ System Message ================================

You are a developer. Your task is to install dependencies and set up a environment that is able to run the tests of the project.

- You start with an initial Docker container based on ubuntu:22.04.
- You interact with a Bash session inside this container.
- Project files are located in /testbed within the container, and your current working directory of bash is already set to /testbed.
- No need to clone the project again.

The final objective is to successfully run the tests of the project.


### Generic Instructions:
- Examine the project structure to understand the build system
- Look for common files like:
  - Makefile, CMakeLists.txt (C/C++)
  - configure, autogen.sh (autotools)
  - setup.py, pyproject.toml, requirements.txt (Python)
  - package.json, yarn.lock, pnpm-lock.yaml (Node.js)
  - Cargo.toml (Rust)
  - pom.xml, build.gradle (Java)
  - go.mod (Go)
  - *.csproj, *.sln (C#)
- Install necessary build tools and dependencies
- Follow the project's README, INSTALL, or BUILD instructions if available
- Try common commands like 'make', 'cmake', './configure', etc.
- Install system dependencies through package manager (apt, yum, apk, etc.)


================================ Human Message =================================


You run in a loop of Thought, Action, Observation.
At the end of the loop you should use Action to stop the loop.

Use Thought to describe your thoughts about the question you have been asked.
Use Action to run one of the actions available to you.
Observation will be the result of running those actions.
> Important Note: Each step, reply with only **one** (Thought, Action) pair.
> Important Note: Do not reply **Observation**, it will be provided by the system.

Your available actions are:

    Command: run a command in the bash, reply with following format, your command should not require sudo or interactive input:
        <command>bash command</command>
        e.g. install build-essential: <command>apt-get install -y build-essential</command>
        e.g. view file content: <command>cat README.md</command>
    Search: search the web for if you need some information, generate query and reply with following format:
        <search>the search query</search>
        e.g. <search>how to fix 'No module named setuptools'</search>
        e.g. <search>how to install python3 on ubuntu</search>
        e.g. <search>how to create development environment for python3</search>
    Stop: stop the setup loop once you think the setup is complete, reply with following format:
        <stop></stop>
    

Observation will be the result of running those actions.


Project Structure: the structure of the project, including files and directories.
Related Files: the content of related files of the project that may help you understand the project.
Thought: you should always think about what to do
Action: <command>your bash command here</command> or <search>your search query</search> or other actions available
Observation: the result of the action
... (this Thought/Action/Observation can repeat N times) ...
Thought: I think the setup should be fine
Action: <stop>stop the setup</stop>
Answer: the final result

Begin!
Project Structure: 📂 
/home/hagon/Project/msb/EnvAgent/git/EnvBench/baseline_RepoLaunch/SWE-bench-Live/launch/playground/EnvPilot_baseline/ggml-org__
llama.cpp-14934/repo
┣━━ 📂 .devops
┣━━ 📂 .github
┣━━ 📂 ci
┣━━ 📂 cmake
┣━━ 📂 common
┣━━ 📂 docs
┣━━ 📂 examples
┣━━ 📂 ggml
┣━━ 📂 gguf-py
┣━━ 📂 grammars
┣━━ 📂 include
┣━━ 📂 licenses
┣━━ 📂 media
┣━━ 📂 models
┣━━ 📂 pocs
┣━━ 📂 prompts
┣━━ 📂 requirements
┣━━ 📂 scripts
┣━━ 📂 src
┣━━ 📂 tests
┣━━ 📂 tools
┣━━ 📂 vendor
┣━━ 📄 .clang-format
┣━━ 📄 .clang-tidy
┣━━ 📄 .dockerignore
┣━━ 📄 .ecrc
┣━━ 📄 .editorconfig
┣━━ 📄 .flake8
┣━━ 📄 .gitmodules
┣━━ 📄 .pre-commit-config.yaml
┣━━ 📄 AUTHORS
┣━━ 📄 build-xcframework.sh
┣━━ 📄 CMakeLists.txt
┣━━ 📄 CMakePresets.json
┣━━ 📄 CODEOWNERS
┣━━ 📄 CONTRIBUTING.md
┣━━ 🐍 convert_hf_to_gguf.py
┣━━ 🐍 convert_hf_to_gguf_update.py
┣━━ 🐍 convert_llama_ggml_to_gguf.py
┣━━ 🐍 convert_lora_to_gguf.py
┣━━ 📄 flake.lock
┣━━ 📄 flake.nix
┣━━ 📄 LICENSE
┣━━ 📄 Makefile
┣━━ 📄 mypy.ini
┣━━ 📄 poetry.lock
┣━━ 📄 pyproject.toml
┣━━ 📄 pyrightconfig.json
┣━━ 📄 README.md
┣━━ 📄 requirements.txt
┗━━ 📄 SECURITY.md

Related Files: ------ BEGIN RELATED FILES ------
File: docs/build.md
```
# Build llama.cpp locally

The main product of this project is the `llama` library. Its C-style interface can be found in [include/llama.h](../include/llama.h).

The project also includes many example programs and tools using the `llama` library. The examples range from simple, minimal code snippets to sophisticated sub-projects such as an OpenAI-compatible HTTP server.

**To get the Code:**

```bash
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
```

The following sections describe how to build with different backends and options.

## CPU Build

Build llama.cpp using `CMake`:

```bash
cmake -B build
cmake --build build --config Release
```

**Notes**:

- For faster compilation, add the `-j` argument to run multiple jobs in parallel, or use a generator that does this automatically such as Ninja. For example, `cmake --build build --config Release -j 8` will run 8 jobs in parallel.
- For faster repeated compilation, install [ccache](https://ccache.dev/)
- For debug builds, there are two cases:

    1. Single-config generators (e.g. default = `Unix Makefiles`; note that they just ignore the `--config` flag):

       ```bash
       cmake -B build -DCMAKE_BUILD_TYPE=Debug
       cmake --build build
       ```

    2. Multi-config generators (`-G` param set to Visual Studio, XCode...):

       ```bash
       cmake -B build -G "Xcode"
       cmake --build build --config Debug
       ```

    For more details and a list of supported generators, see the [CMake documentation](https://cmake.org/cmake/help/latest/manual/cmake-generators.7.html).
- For static builds, add `-DBUILD_SHARED_LIBS=OFF`:
  ```
  cmake -B build -DBUILD_SHARED_LIBS=OFF
  cmake --build build --config Release
  ```

- Building for Windows (x86, x64 and arm64) with MSVC or clang as compilers:
    - Install Visual Studio 2022, e.g. via the [Community Edition](https://visualstudio.microsoft.com/vs/community/). In the installer, select at least the following options (this also automatically installs the required additional tools like CMake,...):
    - Tab Workload: Desktop-development with C++
    - Tab Components (select quickly via search): C++-_CMake_ Tools for Windows, _Git_ for Windows, C++-_Clang_ Compiler for Windows, MS-Build Support for LLVM-Toolset (clang)
    - Please remember to always use a Developer Command Prompt / PowerShell for VS2022 for git, build, test
    - For Windows on ARM (arm64, WoA) build with:
    ```bash
    cmake --preset arm64-windows-llvm-release -D GGML_OPENMP=OFF
    cmake --build build-arm64-windows-llvm-release
    ```
    Building for arm64 can also be done with the MSVC compiler with the build-arm64-windows-MSVC preset, or the standard CMake build instructions. However, note that the MSVC compiler does not support inline ARM assembly code, used e.g. for the accelerated Q4_0_N_M CPU kernels.

    For building with ninja generator and clang compiler as default:
      -set path:set LIB=C:\Program Files (x86)\Windows Kits\10\Lib\10.0.22621.0\um\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.41.34120\lib\x64\uwp;C:\Program Files (x86)\Windows Kits\10\Lib\10.0.22621.0\ucrt\x64
      ```bash
      cmake --preset x64-windows-llvm-release
      cmake --build build-x64-windows-llvm-release
      ```
- Curl usage is enabled by default and can be turned off with `-DLLAMA_CURL=OFF`. Otherwise you need to install development libraries for libcurl.
  - **Debian / Ubuntu:** `sudo apt-get install libcurl4-openssl-dev`  # (or `libcurl4-gnutls-dev` if you prefer GnuTLS)
  - **Fedora / RHEL / Rocky / Alma:** `sudo dnf install libcurl-devel`
  - **Arch / Manjaro:** `sudo pacman -S curl`  # includes libcurl headers

## BLAS Build

Building the program with BLAS support may lead to some performance improvements in prompt processing using batch sizes higher than 32 (the default is 512). Using BLAS doesn't affect the generation performance. There are currently several different BLAS implementations available for build and use:

### Accelerate Framework

This is only available on Mac PCs and it's enabled by default. You can just build using the normal instructions.

### OpenBLAS

This provides BLAS acceleration using only the CPU. Make sure to have OpenBLAS installed on your machine.

- Using `CMake` on Linux:

    ```bash
    cmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS
    cmake --build build --config Release
    ```

### BLIS

Check [BLIS.md](./backend/BLIS.md) for more information.

### Intel oneMKL

Building through oneAPI compilers will make avx_vnni instruction set available for intel processors that do not support avx512 and avx512_vnni. Please note that this build config **does not support Intel GPU**. For Intel GPU support, please refer to [llama.cpp for SYCL](./backend/SYCL.md).

- Using manual oneAPI installation:
  By default, `GGML_BLAS_VENDOR` is set to `Generic`, so if you already sourced intel environment script and assign `-DGGML_BLAS=ON` in cmake, the mkl version of Blas will automatically been selected. Otherwise please install oneAPI and follow the below steps:
    ```bash
    source /opt/intel/oneapi/setvars.sh # You can skip this step if  in oneapi-basekit docker image, only required for manual installation
    cmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=Intel10_64lp -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx -DGGML_NATIVE=ON
    cmake --build build --config Release
    ```

- Using oneAPI docker image:
  If you do not want to source the environment vars and install oneAPI manually, you can also build the code using intel docker container: [oneAPI-basekit](https://hub.docker.com/r/intel/oneapi-basekit). Then, you can use the commands given above.

Check [Optimizing and Running LLaMA2 on Intel® CPU](https://www.intel.com/content/www/us/en/content-details/791610/optimizing-and-running-llama2-on-intel-cpu.html) for more information.

### Other BLAS libraries

Any other BLAS library can be used by setting the `GGML_BLAS_VENDOR` option. See the [CMake documentation](https://cmake.org/cmake/help/latest/module/FindBLAS.html#blas-lapack-vendors) for a list of supported vendors.

## Metal Build

On MacOS, Metal is enabled by default. Using Metal makes the computation run on the GPU.
To disable the Metal build at compile time use the `-DGGML_METAL=OFF` cmake option.

When built with Metal support, you can explicitly disable GPU inference with the `--n-gpu-layers 0` command-line argument.

## SYCL

SYCL is a higher-level programming model to improve programming productivity on various hardware accelerators.

llama.cpp based on SYCL is used to **support Intel GPU** (Data Center Max series, Flex series, Arc series, Built-in GPU and iGPU).

For detailed info, please refer to [llama.cpp for SYCL](./backend/SYCL.md).

## CUDA

This provides GPU acceleration using an NVIDIA GPU. Make sure to have the [CUDA toolkit](https://developer.nvidia.com/cuda-toolkit) installed.

#### Download directly from NVIDIA
You may find the official downloads here: [NVIDIA developer site](https://developer.nvidia.com/cuda-downloads).


#### Compile and run inside a Fedora Toolbox Container
We also have a [guide](./backend/CUDA-FEDORA.md) for setting up CUDA toolkit in a Fedora [toolbox container](https://containertoolbx.org/).

**Recommended for:**
- ***Necessary*** for users of [Atomic Desktops for Fedora](https://fedoraproject.org/atomic-desktops/); such as: [Silverblue](https://fedoraproject.org/atomic-desktops/silverblue/) and [Kinoite](https://fedoraproject.org/atomic-desktops/kinoite/).
  - (there are no supported CUDA packages for these systems)
- ***Necessary*** for users that have a host that is not a: [Supported Nvidia CUDA Release Platform](https://developer.nvidia.com/cuda-downloads).
  - (for example, you may have [Fedora 42 Beta](https://fedoramagazine.org/announcing-fedora-linux-42-beta/) as your your host operating system)
- ***Convenient*** For those running [Fedora Workstation](https://fedoraproject.org/workstation/) or [Fedora KDE Plasma Desktop](https://fedoraproject.org/spins/kde), and want to keep their host system clean.
- *Optionally* toolbox packages are available: [Arch Linux](https://archlinux.org/), [Red Hat Enterprise Linux >= 8.5](https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux), or [Ubuntu](https://ubuntu.com/download)


### Compilation
```bash
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
```

### Override Compute Capability Specifications

If `nvcc` cannot detect your gpu, you may get compile-warnings such as:
 ```text
nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used
```

To override the `native` GPU detection:

#### 1. Take note of the `Compute Capability` of your NVIDIA devices: ["CUDA: Your GPU Compute > Capability"](https://developer.nvidia.com/cuda-gpus).

```text
GeForce RTX 4090      8.9
GeForce RTX 3080 Ti   8.6
GeForce RTX 3070      8.6
```

#### 2. Manually list each varying `Compute Capability` in the `CMAKE_CUDA_ARCHITECTURES` list.

```bash
cmake -B build -DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES="86;89"
```

### Runtime CUDA environmental variables

You may set the [cuda environmental variables](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars) at runtime.

```bash
# Use `CUDA_VISIBLE_DEVICES` to hide the first compute device.
CUDA_VISIBLE_DEVICES="-0" ./build/bin/llama-server --model /srv/models/llama.gguf
```

### Unified Memory

The environment variable `GGML_CUDA_ENABLE_UNIFIED_MEMORY=1` can be used to enable unified memory in Linux. This allows swapping to system RAM instead of crashing when the GPU VRAM is exhausted. In Windows this setting is available in the NVIDIA control panel as `System Memory Fallback`.

### Performance Tuning

The following compilation options are also available to tweak performance:

| Option                        | Legal values           | Default | Description                                                                                                                                                                                                                                                                             |
|-------------------------------|------------------------|---------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| GGML_CUDA_FORCE_MMQ           | Boolean                | false   | Force the use of custom matrix multiplication kernels for quantized models instead of FP16 cuBLAS even if there is no int8 tensor core implementation available (affects V100, CDNA and RDNA3+). MMQ kernels are enabled by default on GPUs with int8 tensor core support. With MMQ force enabled, speed for large batch sizes will be worse but VRAM consumption will be lower.                       |
| GGML_CUDA_FORCE_CUBLAS        | Boolean                | false   | Force the use of FP16 cuBLAS instead of custom matrix multiplication kernels for quantized models                                                                                                                                                                                       |
| GGML_CUDA_F16                 | Boolean                | false   | If enabled, use half-precision floating point arithmetic for the CUDA dequantization + mul mat vec kernels and for the q4_1 and q5_1 matrix matrix multiplication kernels. Can improve performance on relatively recent GPUs.                                                           |
| GGML_CUDA_PEER_MAX_BATCH_SIZE | Positive integer       | 128     | Maximum batch size for which to enable peer access between multiple GPUs. Peer access requires either Linux or NVLink. When using NVLink enabling peer access for larger batch sizes is potentially beneficial.                                                                         |
| GGML_CUDA_FA_ALL_QUANTS       | Boolean                | false   | Compile support for all KV cache quantization type (combinations) for the FlashAttention CUDA kernels. More fine-grained control over KV cache size but compilation takes much longer.                                                                                                  |

## MUSA

This provides GPU acceleration using a Moore Threads GPU. Make sure to have the [MUSA SDK](https://developer.mthreads.com/musa/musa-sdk) installed.

#### Download directly from Moore Threads

You may find the official downloads here: [Moore Threads developer site](https://developer.mthreads.com/sdk/download/musa).

### Compilation

```bash
cmake -B build -DGGML_MUSA=ON
cmake --build build --config Release
```

#### Override Compute Capability Specifications

By default, all supported compute capabilities are enabled. To customize this behavior, you can specify the `MUSA_ARCHITECTURES` option in the CMake command:

```bash
cmake -B build -DGGML_MUSA=ON -DMUSA_ARCHITECTURES="21"
cmake --build build --config Release
```

This configuration enables only compute capability `2.1` (MTT S80) during compilation, which can help reduce compilation time.

#### Compilation options

Most of the compilation options available for CUDA should also be available for MUSA, though they haven't been thoroughly tested yet.

- For static builds, add `-DBUILD_SHARED_LIBS=OFF` and `-DCMAKE_POSITION_INDEPENDENT_CODE=ON`:
  ```
  cmake -B build -DGGML_MUSA=ON \
    -DBUILD_SHARED_LIBS=OFF -DCMAKE_POSITION_INDEPENDENT_CODE=ON
  cmake --build build --config Release
  ```

### Runtime MUSA environmental variables

You may set the [musa environmental variables](https://docs.mthreads.com/musa-sdk/musa-sdk-doc-online/programming_guide/Z%E9%99%84%E5%BD%95/) at runtime.

```bash
# Use `MUSA_VISIBLE_DEVICES` to hide the first compute device.
MUSA_VISIBLE_DEVICES="-0" ./build/bin/llama-server --model /srv/models/llama.gguf
```

### Unified Memory

The environment variable `GGML_CUDA_ENABLE_UNIFIED_MEMORY=1` can be used to enable unified memory in Linux. This allows swapping to system RAM instead of crashing when the GPU VRAM is exhausted.

## HIP

This provides GPU acceleration on HIP-supported AMD GPUs.
Make sure to have ROCm installed.
You can download it from your Linux distro's package manager or from here: [ROCm Quick Start (Linux)](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/tutorial/quick-start.html#rocm-install-quick).

- Using `CMake` for Linux (assuming a gfx1030-compatible AMD GPU):
  ```bash
  HIPCXX="$(hipconfig -l)/clang" HIP_PATH="$(hipconfig -R)" \
      cmake -S . -B build -DGGML_HIP=ON -DAMDGPU_TARGETS=gfx1030 -DCMAKE_BUILD_TYPE=Release \
      && cmake --build build --config Release -- -j 16
  ```

  To enhance flash attention performance on RDNA3+ or CDNA architectures, you can utilize the rocWMMA library by enabling the `-DGGML_HIP_ROCWMMA_FATTN=ON` option. This requires rocWMMA headers to be installed on the build system.

  The rocWMMA library is included by default when installing the ROCm SDK using the `rocm` meta package provided by AMD. Alternatively, if you are not using the meta package, you can install the library using the `rocwmma-dev` or `rocwmma-devel` package, depending on your system's package manager.

  As an alternative, you can manually install the library by cloning it from the official [GitHub repository](https://github.com/ROCm/rocWMMA), checkout the corresponding version tag (e.g. `rocm-6.2.4`) and set `-DCMAKE_CXX_FLAGS="-I<path/to/rocwmma>/library/include/"` in CMake. This also works under Windows despite not officially supported by AMD.

  Note that if you get the following error:
  ```
  clang: error: cannot find ROCm device library; provide its path via '--rocm-path' or '--rocm-device-lib-path', or pass '-nogpulib' to build without ROCm device library
  ```
  Try searching for a directory under `HIP_PATH` that contains the file
  `oclc_abi_version_400.bc`. Then, add the following to the start of the
  command: `HIP_DEVICE_LIB_PATH=<directory-you-just-found>`, so something
  like:
  ```bash
  HIPCXX="$(hipconfig -l)/clang" HIP_PATH="$(hipconfig -p)" \
  HIP_DEVICE_LIB_PATH=<directory-you-just-found> \
      cmake -S . -B build -DGGML_HIP=ON -DAMDGPU_TARGETS=gfx1030 -DCMAKE_BUILD_TYPE=Release \
      && cmake --build build -- -j 16
  ```

- Using `CMake` for Windows (using x64 Native Tools Command Prompt for VS, and assuming a gfx1100-compatible AMD GPU):
  ```bash
  set PATH=%HIP_PATH%\bin;%PATH%
  cmake -S . -B build -G Ninja -DAMDGPU_TARGETS=gfx1100 -DGGML_HIP=ON -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++ -DCMAKE_BUILD_TYPE=Release
  cmake --build build
  ```
  Make sure that `AMDGPU_TARGETS` is set to the GPU arch you want to compile for. The above example uses `gfx1100` that corresponds to Radeon RX 7900XTX/XT/GRE. You can find a list of targets [here](https://llvm.org/docs/AMDGPUUsage.html#processors)
  Find your gpu version string by matching the most significant version information from `rocminfo | grep gfx | head -1 | awk '{print $2}'` with the list of processors, e.g. `gfx1035` maps to `gfx1030`.


The environment variable [`HIP_VISIBLE_DEVICES`](https://rocm.docs.amd.com/en/latest/understand/gpu_isolation.html#hip-visible-devices) can be used to specify which GPU(s) will be used.
If your GPU is not officially supported you can use the environment variable [`HSA_OVERRIDE_GFX_VERSION`] set to a similar GPU, for example 10.3.0 on RDNA2 (e.g. gfx1030, gfx1031, or gfx1035) or 11.0.0 on RDNA3.

### Unified Memory

On Linux it is possible to use unified memory architecture (UMA) to share main memory between the CPU and integrated GPU by setting environment variable `GGML_CUDA_ENABLE_UNIFIED_MEMORY=1`. However, this hurts performance for non-integrated GPUs (but enables working with integrated GPUs).

## Vulkan

### For Windows Users:
**w64devkit**

Download and extract [`w64devkit`](https://github.com/skeeto/w64devkit/releases).

Download and install the [`Vulkan SDK`](https://vulkan.lunarg.com/sdk/home#windows) with the default settings.

Launch `w64devkit.exe` and run the following commands to copy Vulkan dependencies:
```sh
SDK_VERSION=1.3.283.0
cp /VulkanSDK/$SDK_VERSION/Bin/glslc.exe $W64DEVKIT_HOME/bin/
cp /VulkanSDK/$SDK_VERSION/Lib/vulkan-1.lib $W64DEVKIT_HOME/x86_64-w64-mingw32/lib/
cp -r /VulkanSDK/$SDK_VERSION/Include/* $W64DEVKIT_HOME/x86_64-w64-mingw32/include/
cat > $W64DEVKIT_HOME/x86_64-w64-mingw32/lib/pkgconfig/vulkan.pc <<EOF
Name: Vulkan-Loader
Description: Vulkan Loader
Version: $SDK_VERSION
Libs: -lvulkan-1
EOF

```

Switch into the `llama.cpp` directory and build using CMake.
```sh
cmake -B build -DGGML_VULKAN=ON
cmake --build build --config Release
```

**Git Bash MINGW64**

Download and install [`Git-SCM`](https://git-scm.com/downloads/win) with the default settings

Download and install [`Visual Studio Community Edition`](https://visualstudio.microsoft.com/) and make sure you select `C++`

Download and install [`CMake`](https://cmake.org/download/) with the default settings

Download and install the [`Vulkan SDK`](https://vulkan.lunarg.com/sdk/home#windows) with the default settings.

Go into your `llama.cpp` directory and right click, select `Open Git Bash Here` and then run the following commands

```
cmake -B build -DGGML_VULKAN=ON
cmake --build build --config Release
```

Now you can load the model in conversation mode using `Vulkan`

```sh
build/bin/Release/llama-cli -m "[PATH TO MODEL]" -ngl 100 -c 16384 -t 10 -n -2 -cnv
```

**MSYS2**

Install [MSYS2](https://www.msys2.org/) and then run the following commands in a UCRT terminal to install dependencies.
```sh
pacman -S git \
    mingw-w64-ucrt-x86_64-gcc \
    mingw-w64-ucrt-x86_64-cmake \
    mingw-w64-ucrt-x86_64-vulkan-devel \
    mingw-w64-ucrt-x86_64-shaderc
```

Switch into the `llama.cpp` directory and build using CMake.
```sh
cmake -B build -DGGML_VULKAN=ON
cmake --build build --config Release
```

### For Docker users:

You don't need to install the Vulkan SDK. It will be installed inside the container.

```sh
# Build the image
docker build -t llama-cpp-vulkan --target light -f .devops/vulkan.Dockerfile .

# Then, use it:
docker run -it --rm -v "$(pwd):/app:Z" --device /dev/dri/renderD128:/dev/dri/renderD128 --device /dev/dri/card1:/dev/dri/card1 llama-cpp-vulkan -m "/app/models/YOUR_MODEL_FILE" -p "Building a website can be done in 10 simple steps:" -n 400 -e -ngl 33
```

### For Linux users:

First, follow the official LunarG instructions for the installation and setup of the Vulkan SDK in the [Getting Started with the Linux Tarball Vulkan SDK](https://vulkan.lunarg.com/doc/sdk/latest/linux/getting_started.html) guide.

> [!IMPORTANT]
> After completing the first step, ensure that you have used the `source` command on the `setup_env.sh` file inside of the Vulkan SDK in your current terminal session. Otherwise, the build won't work. Additionally, if you close out of your terminal, you must perform this step again if you intend to perform a build. However, there are ways to make this persistent. Refer to the Vulkan SDK guide linked in the first step for more information about any of this.

Second, after verifying that you have followed all of the SDK installation/setup steps, use this command to make sure before proceeding:
```bash
vulkaninfo
```

Then, assuming you have `cd` into your llama.cpp folder and there are no errors with running `vulkaninfo`, you can proceed to build llama.cpp using the CMake commands below:
```bash
cmake -B build -DGGML_VULKAN=1
cmake --build build --config Release
```

Finally, after finishing your build, you should be able to do something like this:
```bash
# Test the output binary
# "-ngl 99" should offload all of the layers to GPU for most (if not all) models.
./build/bin/llama-cli -m "PATH_TO_MODEL" -p "Hi you how are you" -ngl 99

# You should see in the output, ggml_vulkan detected your GPU. For example:
# ggml_vulkan: Using Intel(R) Graphics (ADL GT2) | uma: 1 | fp16: 1 | warp size: 32
```

## CANN
This provides NPU acceleration using the AI cores of your Ascend NPU. And [CANN](https://www.hiascend.com/en/software/cann) is a hierarchical APIs to help you to quickly build AI applications and service based on Ascend NPU.

For more information about Ascend NPU in [Ascend Community](https://www.hiascend.com/en/).

Make sure to have the CANN toolkit installed. You can download it from here: [CANN Toolkit](https://www.hiascend.com/developer/download/community/result?module=cann)

Go to `llama.cpp` directory and build using CMake.
```bash
cmake -B build -DGGML_CANN=on -DCMAKE_BUILD_TYPE=release
cmake --build build --config release
```

You can test with:

```bash
./build/bin/llama-cli -m PATH_TO_MODEL -p "Building a website can be done in 10 steps:" -ngl 32
```

If the following info is output on screen, you are using `llama.cpp` with the CANN backend:
```bash
llm_load_tensors:       CANN model buffer size = 13313.00 MiB
llama_new_context_with_model:       CANN compute buffer size =  1260.81 MiB
```

For detailed info, such as model/device supports, CANN install, please refer to [llama.cpp for CANN](./backend/CANN.md).

## Arm® KleidiAI™
KleidiAI is a library of optimized microkernels for AI workloads, specifically designed for Arm CPUs. These microkernels enhance performance and can be enabled for use by the CPU backend.

To enable KleidiAI, go to the llama.cpp directory and build using CMake
```bash
cmake -B build -DGGML_CPU_KLEIDIAI=ON
cmake --build build --config Release
```
You can verify that KleidiAI is being used by running
```bash
./build/bin/llama-cli -m PATH_TO_MODEL -p "What is a car?"
```
If KleidiAI is enabled, the ouput will contain a line similar to:
```
load_tensors: CPU_KLEIDIAI model buffer size =  3474.00 MiB
```
KleidiAI's microkernels implement optimized tensor operations using Arm CPU features such as dotprod, int8mm and SME. llama.cpp selects the most efficient kernel based on runtime CPU feature detection. However, on platforms that support SME, you must manually enable SME microkernels by setting the environment variable `GGML_KLEIDIAI_SME=1`.

Depending on your build target, other higher priority backends may be enabled by default. To ensure the CPU backend is used, you must disable the higher priority backends either at compile time, e.g. -DGGML_METAL=OFF, or during run-time using the command line option `--device none`.

## OpenCL

This provides GPU acceleration through OpenCL on recent Adreno GPU.
More information about OpenCL backend can be found in [OPENCL.md](./backend/OPENCL.md) for more information.

### Android

Assume NDK is available in `$ANDROID_NDK`. First, install OpenCL headers and ICD loader library if not available,

```sh
mkdir -p ~/dev/llm
cd ~/dev/llm

git clone https://github.com/KhronosGroup/OpenCL-Headers && \
cd OpenCL-Headers && \
cp -r CL $ANDROID_NDK/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/include

cd ~/dev/llm

git clone https://github.com/KhronosGroup/OpenCL-ICD-Loader && \
cd OpenCL-ICD-Loader && \
mkdir build_ndk && cd build_ndk && \
cmake .. -G Ninja -DCMAKE_BUILD_TYPE=Release \
  -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake \
  -DOPENCL_ICD_LOADER_HEADERS_DIR=$ANDROID_NDK/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/include \
  -DANDROID_ABI=arm64-v8a \
  -DANDROID_PLATFORM=24 \
  -DANDROID_STL=c++_shared && \
ninja && \
cp libOpenCL.so $ANDROID_NDK/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/lib/aarch64-linux-android
```

Then build llama.cpp with OpenCL enabled,

```sh
cd ~/dev/llm

git clone https://github.com/ggml-org/llama.cpp && \
cd llama.cpp && \
mkdir build-android && cd build-android

cmake .. -G Ninja \
  -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake \
  -DANDROID_ABI=arm64-v8a \
  -DANDROID_PLATFORM=android-28 \
  -DBUILD_SHARED_LIBS=OFF \
  -DGGML_OPENCL=ON

ninja
```

### Windows Arm64

First, install OpenCL headers and ICD loader library if not available,

```powershell
mkdir -p ~/dev/llm

cd ~/dev/llm
git clone https://github.com/KhronosGroup/OpenCL-Headers && cd OpenCL-Headers
mkdir build && cd build
cmake .. -G Ninja `
  -DBUILD_TESTING=OFF `
  -DOPENCL_HEADERS_BUILD_TESTING=OFF `
  -DOPENCL_HEADERS_BUILD_CXX_TESTS=OFF `
  -DCMAKE_INSTALL_PREFIX="$HOME/dev/llm/opencl"
cmake --build . --target install

cd ~/dev/llm
git clone https://github.com/KhronosGroup/OpenCL-ICD-Loader && cd OpenCL-ICD-Loader
mkdir build && cd build
cmake .. -G Ninja `
  -DCMAKE_BUILD_TYPE=Release `
  -DCMAKE_PREFIX_PATH="$HOME/dev/llm/opencl" `
  -DCMAKE_INSTALL_PREFIX="$HOME/dev/llm/opencl"
cmake --build . --target install
```

Then build llama.cpp with OpenCL enabled,

```powershell
cmake .. -G Ninja `
  -DCMAKE_TOOLCHAIN_FILE="$HOME/dev/llm/llama.cpp/cmake/arm64-windows-llvm.cmake" `
  -DCMAKE_BUILD_TYPE=Release `
  -DCMAKE_PREFIX_PATH="$HOME/dev/llm/opencl" `
  -DBUILD_SHARED_LIBS=OFF `
  -DGGML_OPENCL=ON
ninja
```

## Android

To read documentation for how to build on Android, [click here](./android.md)

## WebGPU [In Progress]

The WebGPU backend relies on [Dawn](https://dawn.googlesource.com/dawn). Follow the instructions [here](https://dawn.googlesource.com/dawn/+/refs/heads/main/docs/quickstart-cmake.md) to install Dawn locally so that llama.cpp can find it using CMake. The currrent implementation is up-to-date with Dawn commit `bed1a61`.

In the llama.cpp directory, build with CMake:

```
cmake -B build -DGGML_WEBGPU=ON
cmake --build build --config Release
```

### Browser Support

WebGPU allows cross-platform access to the GPU from supported browsers. We utilize [Emscripten](https://emscripten.org/) to compile ggml's WebGPU backend to WebAssembly. Emscripten does not officially support WebGPU bindings yet, but Dawn currently maintains its own WebGPU bindings called emdawnwebgpu.

Follow the instructions [here](https://dawn.googlesource.com/dawn/+/refs/heads/main/src/emdawnwebgpu/) to download or build the emdawnwebgpu package (Note that it might be safer to build the emdawbwebgpu package locally, so that it stays in sync with the version of Dawn you have installed above). When building using CMake, the path to the emdawnwebgpu port file needs to be set with the flag `EMDAWNWEBGPU_DIR`.

## IBM Z & LinuxONE

To read documentation for how to build on IBM Z & LinuxONE, [click here](./build-s390x.md)

## Notes about GPU-accelerated backends

The GPU may still be used to accelerate some parts of the computation even when using the `-ngl 0` option. You can fully disable GPU acceleration by using `--device none`.

In most cases, it is possible to build and use multiple backends at the same time. For example, you can build llama.cpp with both CUDA and Vulkan support by using the `-DGGML_CUDA=ON -DGGML_VULKAN=ON` options with CMake. At runtime, you can specify which backend devices to use with the `--device` option. To see a list of available devices, use the `--list-devices` option.

Backends can be built as dynamic libraries that can be loaded dynamically at runtime. This allows you to use the same llama.cpp binary on different machines with different GPUs. To enable this feature, use the `GGML_BACKEND_DL` option when building.

```
File: CMakeLists.txt
```
cmake_minimum_required(VERSION 3.14) # for add_link_options and implicit target directories.
project("llama.cpp" C CXX)
include(CheckIncludeFileCXX)

#set(CMAKE_WARN_DEPRECATED YES)
set(CMAKE_WARN_UNUSED_CLI YES)

set(CMAKE_EXPORT_COMPILE_COMMANDS ON)

if (NOT XCODE AND NOT MSVC AND NOT CMAKE_BUILD_TYPE)
    set(CMAKE_BUILD_TYPE Release CACHE STRING "Build type" FORCE)
    set_property(CACHE CMAKE_BUILD_TYPE PROPERTY STRINGS "Debug" "Release" "MinSizeRel" "RelWithDebInfo")
endif()

# Add path to modules
list(APPEND CMAKE_M
... (truncated due to size)

================================ Human Message =================================


The commands you have run:```
apt-get update && apt-get install -y cmake build-essential git, cmake -B build, cmake -B build -DLLAMA_CURL=OFF, cmake --build build --config Release, ls build/bin/ | grep test, ./build/bin/test-gguf```
Following are the last 5 messages:

================================ Human Message =================================

Observation:
cmake --build build --config Release
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  4%] Linking CXX shared library ../../bin/libggml-base.so
[  4%] Built target ggml-base
[  5%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.so
[ 11%] Built target ggml-cpu
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 11%] Linking CXX shared library ../../bin/libggml.so
[ 11%] Built target ggml
[ 11%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 12%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 12%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 12%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 13%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 13%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-cparams.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache-unified.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache-unified-iswa.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-model-saver.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp....stripped due to length....
.cpp.o
[ 82%] Linking CXX executable ../../bin/llama-diffusion-cli
[ 82%] Built target llama-diffusion-cli
[ 82%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 83%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 83%] Built target llama-convert-llama2c-to-ggml
[ 84%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-vdot
[ 84%] Built target llama-vdot
[ 84%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-q8dot
[ 84%] Built target llama-q8dot
[ 84%] Building CXX object tools/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-batched-bench
[ 84%] Built target llama-batched-bench
[ 84%] Building CXX object tools/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-gguf-split
[ 85%] Built target llama-gguf-split
[ 85%] Building CXX object tools/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-imatrix
[ 86%] Built target llama-imatrix
[ 87%] Building CXX object tools/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-bench
[ 87%] Built target llama-bench
[ 88%] Building CXX object tools/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-cli
[ 88%] Built target llama-cli
[ 88%] Building CXX object tools/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-perplexity
[ 89%] Built target llama-perplexity
[ 90%] Building CXX object tools/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-quantize
[ 90%] Built target llama-quantize
[ 91%] Generating loading.html.hpp
[ 91%] Generating index.html.gz.hpp
[ 91%] Building CXX object tools/server/CMakeFiles/llama-server.dir/server.cpp.o
[ 92%] Linking CXX executable ../../bin/llama-server
[ 92%] Built target llama-server
[ 92%] Building CXX object tools/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 93%] Building CXX object tools/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 93%] Linking CXX executable ../../bin/llama-run
[ 93%] Built target llama-run
[ 93%] Building CXX object tools/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-tokenize
[ 94%] Built target llama-tokenize
[ 94%] Building CXX object tools/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-tts
[ 94%] Built target llama-tts
[ 94%] Building CXX object tools/mtmd/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-llava-cli
[ 94%] Built target llama-llava-cli
[ 94%] Building CXX object tools/mtmd/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-gemma3-cli
[ 95%] Built target llama-gemma3-cli
[ 96%] Building CXX object tools/mtmd/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Built target llama-minicpmv-cli
[ 97%] Building CXX object tools/mtmd/CMakeFiles/llama-qwen2vl-cli.dir/deprecation-warning.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Built target llama-qwen2vl-cli
[ 98%] Building CXX object tools/mtmd/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-mtmd-cli
[ 98%] Built target llama-mtmd-cli
[ 98%] Building CXX object tools/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 99%] Linking CXX executable ../../bin/llama-cvector-generator
[ 99%] Built target llama-cvector-generator
[100%] Building CXX object tools/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[100%] Linking CXX executable ../../bin/llama-export-lora
[100%] Built target llama-export-lora


root@36d223b37fb8:/testbed $

exit code: 0

================================== Ai Message ==================================

Thought: The build process completed successfully. Now we can run some tests to verify the installation. Looking at the project structure, there's a 'tests' directory, so we should be able to run tests from there. First, let's check what test executables were built.

Action: <command>ls build/bin/ | grep test</command>
================================ Human Message =================================

Observation:
ls build/bin/ | grep test
test-arg-parser
test-autorelease
test-backend-ops
test-barrier
test-c
test-chat
test-chat-parser
test-chat-template
test-gbnf-validator
test-gguf
test-grammar-integration
test-grammar-parser
test-json-partial
test-json-schema-to-grammar
test-llama-grammar
test-log
test-model-load-cancel
test-mtmd-c-api
test-quantize-fns
test-quantize-perf
test-quantize-stats
test-regex-partial
test-rope
test-sampling
test-thread-safety
test-tokenizer-0
test-tokenizer-1-bpe
test-tokenizer-1-spm


root@36d223b37fb8:/testbed $

exit code: 0

================================== Ai Message ==================================

Thought: Several test executables were built successfully. Let's run one of the basic tests to verify the installation works correctly. We'll start with `test-gguf` which tests the GGUF file format functionality.

Action: <command>./build/bin/test-gguf</command>
================================ Human Message =================================

Observation:
./build/bin/test-gguf

test_handcrafted_file: handcrafted_file_type=HEADER_BAD_MAGIC
gguf_init_from_file_impl: invalid magic characters: 'FUGG', expected 'GGUF'
test_handcrafted_file:   - context_null: OK

test_handcrafted_file: handcrafted_file_type=HEADER_BAD_VERSION_0
gguf_init_from_file_impl: bad GGUF version: 0
gguf_init_from_file_impl: failed to read header
test_handcrafted_file:   - context_null: OK

test_handcrafted_file: handcrafted_file_type=HEADER_BAD_VERSION_1
gguf_init_from_file_impl: GGUFv1 is no longer supported, please use a more up-to-date version
gguf_init_from_file_impl: failed to read header
test_handcrafted_file:   - context_null: OK

test_handcrafted_file: handcrafted_file_type=HEADER_BAD_VERSION_FUTURE
gguf_init_from_file_impl: this GGUF file is version 4 but this software only supports up to version 3
gguf_init_from_file_impl: failed to read header
test_handcrafted_file:   - context_null: OK

test_handcrafted_file: handcrafted_file_type=HEADER_BAD_N_KV
gguf_init_from_file_impl: number of key value pairs is -1 but must be in [0, 209622091746699450]
gguf_init_from_file_impl: failed to read header
test_handcrafted_file:   - context_null: OK

test_handcrafted_file: handcrafted_file_type=HEADER_BAD_N_TENSORS
gguf_init_from_file_impl: number of tensors is -1 but must be in [0, 53624256028225440]
gguf_init_from_file_impl: failed to read header
test_handcrafted_file:   - context_null: OK

test_handcrafted_file: handcrafted_file_type=HEADER_EMPTY
test_handcrafted_file:   - context_not_null: OK
test_handcrafted_file:   - check_header: OK

test_handcrafted_file: handcrafted_file_type=KV_BAD_KEY_SIZE
gguf_init_from_file_impl: encountered length_error while reading key 0
gguf_init_from_file_impl: failed to read key-value pairs
test_handcrafted_file:   - context_null: OK

test_handcrafted_file: handcrafted_file_type=KV_BAD_TYPE
gguf_init_from_file_impl: key 'my_key_0' has invalid GGUF type 13
gguf_init_from_file_impl: failed to read key-value pairs
test_handcrafted_file:   - context_null: OK

test_handcrafted_file: handcrafted_file_type=KV_DUPLICATE_KEY
gguf_init_from_file_impl: duplicate key 'my_key_0' for tensors 0 and 1 
gguf_init_from_file_impl: failed to read key-value pairs
test_handcrafted_file:   - context_null: OK

test_handcrafted_file: handcrafted_file_type=KV_BAD_ALIGN
gguf_init_from_file_impl: alignment 13 is not a power of 2
test_handcrafted_file:   - context_null: OK

test_handcrafted_file: handcrafted_file_type=KV_RANDOM_KV
test_handcrafted_file:   - context_not_null: OK
test_handcrafted_file:   - check_header: OK
test_handcrafted_file:   - check_kv: OK

test_handcrafted_file: handcrafted_file_type=TENSORS_BAD_NAME_SIZE
gguf_init_from_file_impl: tensor name 0 is too long: 87 >= 64
gguf_init_from_file_impl: failed to read tensor info
test_handcrafted_file:   - context_null: OK

test_handcrafted_file: handcrafted_file_type=TENSORS_BAD_N_DIMS
gguf_init_from_file_impl: tensor 'my_tensor_0' has invalid number of dimensions: 5 > 4
gguf_init_from_file_impl: failed to read tensor info
test_handcrafted_file:   - context_null: OK

test_handcrafted_file: handcrafted_file_type=TENSORS_BAD_SHAPE
gguf_init_from_file_impl: tensor 'my_tensor_0' dimension 0 has invalid number of elements: -1 < 0
gguf_init_from_file_impl: failed to read tensor info
test_handcrafted_file:   - context_null: OK

test_handcrafted_file: handcrafted_file_type=TENSORS_NE_TOO_BIG
gguf_init_from_file_impl: total number of elements in tensor 'my_tensor_0' with shape (8589934588, 8589934588, 1, 1) is >= 9223372036854775807
gguf_init_from_file_impl: failed to read tensor info
test_handcrafted_file:   - context_null: OK

test_handcrafted_file: handcrafted_file_type=TENSORS_BAD_TYPE
gguf_init_from_file_impl: tensor 'my_tensor_0' has invalid ggml type 39 (NONE)
gguf_init_from_file_impl: failed to read tensor info
test_handcrafted_file:   - context_null: OK

test_handcrafted_file: handcrafted_file_type=TENSORS_BAD_OFFSET
gguf_init_from_file_impl: tensor 'my_tensor_0' has offset 18446744073709551615, expected 0
gguf_init_from_file_impl: failed to read tensor data
test_handcrafted_file:   - context_null: OK

test_handcrafted_file: handcrafted_file_type=TENSORS_DUPLICATE_NAME
gguf_init_from_file_impl: duplicate tensor name 'my_tensor' for tensors 0 and 1
gguf_init_from_file_impl: failed to read tensor info
test_handcrafted_file:   - context_null: OK

test_handcrafted_file: handcrafted_file_type=TENSORS_BAD_ALIGN
gguf_init_from_file_impl: alignment 13 is not a power of 2
test_handcrafted_file:   - context_null: OK

test_handcrafted_file: handcrafted_file_type=TENSORS_INCONSISTENT_ALIGN
gguf_init_from_file_impl: tensor 'my_tensor_2' has offset 524, expected 544
gguf_init_from_file_impl: failed to read tensor data
test_handcrafted_file:   - context_null: OK

test_handcrafted_file: handcrafted_file_type=TENSORS_SUCCESS
test_handcrafted_file:   - context_not_null: OK
test_handcrafted_file:   - check_header: OK
test_handcrafted_file:   - check_kv: OK
test_handcrafted_file:   - check_tensors: OK

test_handcrafted_file: handcrafted_file_type=TENSORS_CUSTOM_ALIGN
test_handcrafted_file:   - context_not_null: OK
test_handcrafted_file:   - check_header: OK
test_handcrafted_file:   - check_kv: OK
test_handcrafted_file:   - check_tensors: OK

test_handcrafted_file: handcrafted_file_type=DATA_NOT_ENOUGH_DATA
gguf_init_from_file_impl: failed to read tensor data binary blob
test_handcrafted_file:   - context_null: OK
test_handcrafted_file:   - no_dangling_ggml_context_pointer: OK

test_handcrafted_file: handcrafted_file_type=DATA_BAD_ALIGN
gguf_init_from_file_impl: alignment 13 is not a power of 2
test_handcrafted_file:   - context_null: OK
test_handcrafted_file:   - no_dangling_ggml_context_pointer: OK

test_handcrafted_file: handcrafted_file_type=DATA_INCONSISTENT_ALIGN
gguf_init_from_file_impl: tensor 'my_tensor_2' has offset 524, expected 544
gguf_init_from_file_impl: failed to read tensor data
test_handcrafted_file:   - context_null: OK
test_handcrafted_file:   - no_dangling_ggml_context_pointer: OK

test_handcrafted_file: handcrafted_file_type=DATA_SUCCESS
test_handcrafted_file:   - context_not_null: OK
test_handcrafted_file:   - check_header: OK
test_handcrafted_file:   - check_kv: OK
test_handcrafted_file:   - check_tensors: OK
test_handcrafted_file:   - check_tensor_data: OK

test_handcrafted_file: handcrafted_file_type=DATA_CUSTOM_ALIGN
test_handcrafted_file:   - context_not_null: OK
test_handcrafted_file:   - check_header: OK
test_handcrafted_file:   - check_kv: OK
test_handcrafted_file:   - check_tensors: OK
test_handcrafted_file:   - check_tensor_data: OK

test_roundtrip: device=13th Gen Intel(R) Core(TM) i7-13650HX, backend=CPU, only_meta=yes
test_roundtrip: same_version: OK
test_roundtrip: same_n_kv: OK
test_roundtrip: same_n_tensors: OK
test_roundtrip: all_orig_kv_in_read: OK
test_roundtrip: all_read_kv_in_orig: OK
test_roundtrip: all_orig_tensors_in_read: OK
test_roundtrip: all_read_tensors_in_orig: OK

test_roundtrip: device=13th Gen Intel(R) Core(TM) i7-13650HX, backend=CPU, only_meta=no
test_roundtrip: same_version: OK
test_roundtrip: same_n_kv: OK
test_roundtrip: same_n_tensors: OK
test_roundtrip: all_orig_kv_in_read: OK
test_roundtrip: all_read_kv_in_orig: OK
test_roundtrip: all_orig_tensors_in_read: OK
test_roundtrip: all_read_tensors_in_orig: OK
test_roundtrip: same_tensor_data: OK

test_gguf_set_kv: device=13th Gen Intel(R) Core(TM) i7-13650HX, backend=CPU
test_gguf_set_kv: same_n_kv: OK
test_gguf_set_kv: all_kv_0_in_1: OK
test_gguf_set_kv: all_kv_0_in_2: OK
test_gguf_set_kv: same_n_kv_after_double_copy: OK
test_gguf_set_kv: all_kv_1_in_0_after_double_copy: OK

68/68 tests passed
OK


root@36d223b37fb8:/testbed $

exit code: 0

##### LLM OUTPUT #####
================================== Ai Message ==================================

Thought: The test-gguf test passed successfully with all 68 tests completed successfully. This confirms that the GGUF file format functionality is working correctly. Since the basic tests are passing, the environment setup appears to be correct. We can now stop the setup process.

Action: <stop></stop>